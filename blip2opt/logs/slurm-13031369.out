WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
loss DRSL3 b=1e-06 start=0 end=6loss DRSL3 b=1e-06 start=0 end=6

loss DRSL3 b=1e-06 start=0 end=6loss DRSL3 b=1e-06 start=0 end=6

| distributed init (rank 2, world 4): env://| distributed init (rank 3, world 4): env://| distributed init (rank 0, world 4): env://| distributed init (rank 1, world 4): env://



[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
2023-08-21 15:28:59,233 [INFO] 
=====  Running Parameters    =====
2023-08-21 15:28:59,234 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 32,
    "batch_size_train": 12,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "inference_method": "generate",
    "init_lr": 1e-05,
    "lr_layer_decay": 0.95,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 2,
    "max_len": 10,
    "min_len": 1,
    "min_lr": 0,
    "num_beams": 5,
    "num_workers": 4,
    "output_dir": "output_vqaDRSL/BLIP2/DRSL3_6_0_6",
    "prompt": "Question: {} Short answer:",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "vqa",
    "train_splits": [
        "train"
    ],
    "warmup_lr": 1e-08,
    "warmup_steps": 1000,
    "weight_decay": 0.05,
    "world_size": 4
}
2023-08-21 15:28:59,234 [INFO] 
======  Dataset Attributes  ======
2023-08-21 15:28:59,234 [INFO] 
======== vg_vqa =======
2023-08-21 15:28:59,235 [INFO] {
    "build_info": {
        "annotations": {
            "train": {
                "storage": "vg/annotations/vg_qa.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/visual_genome/vg_qa.json"
            }
        },
        "images": {
            "storage": "vg/images/"
        }
    },
    "data_type": "images",
    "text_processor": {
        "train": {
            "name": "blip_question"
        }
    },
    "vis_processor": {
        "train": {
            "image_size": 400,
            "name": "blip_image_train"
        }
    }
}
2023-08-21 15:28:59,235 [INFO] 
======  Model Attributes  ======
2023-08-21 15:28:59,236 [INFO] {
    "arch": "blip2_opt",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 400,
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "pretrain_opt2.7b",
    "num_query_token": 32,
    "opt_model": "facebook/opt-2.7b",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth",
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_model": "eva_clip_g",
    "vit_precision": "fp32"
}
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/vg/annotations/vg_qa.json
2023-08-21 15:28:59,250 [INFO] Building datasets...
BlipQuestionProcessor
Position interpolate from 16x16 to 28x28
2023-08-21 15:29:37,395 [INFO] freeze vision encoder
pretrain_path: https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-21 15:33:00,735 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-21 15:33:00,754 [INFO] Start training
2023-08-21 15:33:22,169 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-08-21 15:33:22,170 [INFO] Loaded 145395 records for train split from the dataset.
2023-08-21 15:33:22,335 [INFO] number of trainable parameters: 107133696
2023-08-21 15:33:22,345 [INFO] Start training epoch 0, 3029 iters per inner epoch.
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
Train: data epoch: [0]  [   0/3029]  eta: 21:14:04  lr: 0.000000  loss: 2.7844  time: 25.2376  data: 0.0000  max mem: 12766
2023-08-21 15:33:47,649 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  50/3029]  eta: 4:38:44  lr: 0.000001  loss: 2.6769  time: 5.2296  data: 0.0000  max mem: 14218
Train: data epoch: [0]  [ 100/3029]  eta: 4:24:15  lr: 0.000001  loss: 2.5699  time: 5.2220  data: 0.0000  max mem: 14218
Train: data epoch: [0]  [ 150/3029]  eta: 4:16:33  lr: 0.000002  loss: 2.5211  time: 5.2035  data: 0.0000  max mem: 14288
Train: data epoch: [0]  [ 200/3029]  eta: 4:10:40  lr: 0.000002  loss: 1.7766  time: 5.2188  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 250/3029]  eta: 4:05:17  lr: 0.000003  loss: 1.8232  time: 5.2245  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 300/3029]  eta: 4:00:16  lr: 0.000003  loss: 2.2232  time: 5.2042  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 350/3029]  eta: 3:55:43  lr: 0.000004  loss: 2.0644  time: 5.2850  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 400/3029]  eta: 3:51:18  lr: 0.000004  loss: 2.3662  time: 5.2635  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 450/3029]  eta: 3:46:53  lr: 0.000005  loss: 2.0125  time: 5.2644  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 500/3029]  eta: 3:42:29  lr: 0.000005  loss: 2.0672  time: 5.2912  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 550/3029]  eta: 3:38:01  lr: 0.000006  loss: 1.5467  time: 5.2809  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 600/3029]  eta: 3:33:40  lr: 0.000006  loss: 2.1670  time: 5.3210  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 650/3029]  eta: 3:29:19  lr: 0.000007  loss: 2.4753  time: 5.3100  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 700/3029]  eta: 3:24:54  lr: 0.000007  loss: 1.8882  time: 5.2602  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 750/3029]  eta: 3:20:31  lr: 0.000008  loss: 2.2159  time: 5.2986  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 800/3029]  eta: 3:16:09  lr: 0.000008  loss: 1.8739  time: 5.3073  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 850/3029]  eta: 3:11:45  lr: 0.000009  loss: 2.3074  time: 5.2896  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 900/3029]  eta: 3:07:23  lr: 0.000009  loss: 2.2529  time: 5.3042  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 950/3029]  eta: 3:02:57  lr: 0.000010  loss: 2.2795  time: 5.2731  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1000/3029]  eta: 2:58:33  lr: 0.000010  loss: 1.7559  time: 5.2739  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1050/3029]  eta: 2:54:08  lr: 0.000010  loss: 2.0269  time: 5.2665  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1100/3029]  eta: 2:49:45  lr: 0.000010  loss: 2.1643  time: 5.2876  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1150/3029]  eta: 2:45:20  lr: 0.000010  loss: 2.0555  time: 5.2510  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1200/3029]  eta: 2:40:57  lr: 0.000010  loss: 2.0423  time: 5.2980  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1250/3029]  eta: 2:36:32  lr: 0.000010  loss: 2.2819  time: 5.2577  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1300/3029]  eta: 2:32:07  lr: 0.000010  loss: 2.0217  time: 5.2582  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1350/3029]  eta: 2:27:43  lr: 0.000010  loss: 1.8636  time: 5.2578  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1400/3029]  eta: 2:23:19  lr: 0.000010  loss: 1.9189  time: 5.2527  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1450/3029]  eta: 2:18:56  lr: 0.000010  loss: 1.9730  time: 5.3116  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1500/3029]  eta: 2:14:33  lr: 0.000010  loss: 2.1049  time: 5.2841  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1550/3029]  eta: 2:10:08  lr: 0.000010  loss: 1.9070  time: 5.2959  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1600/3029]  eta: 2:05:44  lr: 0.000010  loss: 2.1490  time: 5.2706  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1650/3029]  eta: 2:01:19  lr: 0.000010  loss: 1.9743  time: 5.2623  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1700/3029]  eta: 1:56:55  lr: 0.000010  loss: 2.4383  time: 5.2055  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1750/3029]  eta: 1:52:29  lr: 0.000010  loss: 1.9965  time: 5.2285  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1800/3029]  eta: 1:48:03  lr: 0.000010  loss: 1.7598  time: 5.2188  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1850/3029]  eta: 1:43:37  lr: 0.000010  loss: 1.9139  time: 5.2200  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1900/3029]  eta: 1:39:12  lr: 0.000010  loss: 2.2897  time: 5.2222  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1950/3029]  eta: 1:34:47  lr: 0.000010  loss: 1.8658  time: 5.2227  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2000/3029]  eta: 1:30:22  lr: 0.000010  loss: 1.6948  time: 5.2292  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2050/3029]  eta: 1:25:57  lr: 0.000010  loss: 2.1569  time: 5.2086  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2100/3029]  eta: 1:21:32  lr: 0.000010  loss: 2.0837  time: 5.2237  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2150/3029]  eta: 1:17:08  lr: 0.000010  loss: 2.3083  time: 5.2048  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2200/3029]  eta: 1:12:44  lr: 0.000010  loss: 2.2384  time: 5.2139  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2250/3029]  eta: 1:08:20  lr: 0.000010  loss: 1.9622  time: 5.2531  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2300/3029]  eta: 1:03:56  lr: 0.000010  loss: 2.4274  time: 5.2291  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2350/3029]  eta: 0:59:33  lr: 0.000010  loss: 2.3031  time: 5.2260  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2400/3029]  eta: 0:55:09  lr: 0.000010  loss: 2.1393  time: 5.2381  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2450/3029]  eta: 0:50:45  lr: 0.000010  loss: 1.8358  time: 5.2143  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2500/3029]  eta: 0:46:22  lr: 0.000010  loss: 2.0808  time: 5.2149  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2550/3029]  eta: 0:41:59  lr: 0.000010  loss: 2.1297  time: 5.2375  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2600/3029]  eta: 0:37:35  lr: 0.000010  loss: 2.1297  time: 5.2080  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2650/3029]  eta: 0:33:12  lr: 0.000010  loss: 1.8535  time: 5.2323  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2700/3029]  eta: 0:28:49  lr: 0.000010  loss: 2.2139  time: 5.2331  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2750/3029]  eta: 0:24:26  lr: 0.000010  loss: 1.8053  time: 5.2171  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2800/3029]  eta: 0:20:03  lr: 0.000010  loss: 1.8146  time: 5.2165  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2850/3029]  eta: 0:15:40  lr: 0.000010  loss: 1.6969  time: 5.2204  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2900/3029]  eta: 0:11:17  lr: 0.000010  loss: 1.7656  time: 5.2101  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2950/3029]  eta: 0:06:55  lr: 0.000010  loss: 2.2380  time: 5.2241  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [3000/3029]  eta: 0:02:32  lr: 0.000010  loss: 2.0348  time: 5.2175  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [3028/3029]  eta: 0:00:05  lr: 0.000010  loss: 1.9420  time: 5.2141  data: 0.0000  max mem: 14563
Train: data epoch: [0] Total time: 4:25:11 (5.2530 s / it)
2023-08-21 19:58:33,740 [INFO] Averaged stats: lr: 0.0000  loss: 2.0496
2023-08-21 19:58:33,793 [INFO] No validation splits found.
2023-08-21 19:58:33,848 [INFO] Saving checkpoint at epoch 0 to /public/home/mswanghao/TorchProject/lavis/lavis/output_vqaDRSL/BLIP2/DRSL3_6_0_6/20230821152/checkpoint_0.pth.
2023-08-21 19:58:38,497 [INFO] Start training
2023-08-21 19:58:38,541 [INFO] Start training epoch 1, 3029 iters per inner epoch.
Train: data epoch: [1]  [   0/3029]  eta: 9:41:18  lr: 0.000005  loss: 1.8366  time: 11.5147  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [  50/3029]  eta: 4:24:57  lr: 0.000005  loss: 1.7765  time: 5.2108  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 100/3029]  eta: 4:17:23  lr: 0.000005  loss: 1.7493  time: 5.2410  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 150/3029]  eta: 4:12:12  lr: 0.000005  loss: 2.4653  time: 5.2106  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 200/3029]  eta: 4:07:27  lr: 0.000005  loss: 2.1338  time: 5.2387  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 250/3029]  eta: 4:02:48  lr: 0.000005  loss: 1.7252  time: 5.2365  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 300/3029]  eta: 3:58:13  lr: 0.000005  loss: 2.0254  time: 5.2068  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 350/3029]  eta: 3:53:38  lr: 0.000005  loss: 1.8710  time: 5.1862  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 400/3029]  eta: 3:49:14  lr: 0.000005  loss: 1.9979  time: 5.2301  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 450/3029]  eta: 3:44:51  lr: 0.000005  loss: 1.7386  time: 5.2143  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 500/3029]  eta: 3:40:25  lr: 0.000005  loss: 2.1418  time: 5.2293  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 550/3029]  eta: 3:36:00  lr: 0.000005  loss: 1.9858  time: 5.2163  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 600/3029]  eta: 3:31:34  lr: 0.000005  loss: 2.2865  time: 5.1932  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 650/3029]  eta: 3:27:10  lr: 0.000005  loss: 1.9610  time: 5.1898  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 700/3029]  eta: 3:22:47  lr: 0.000005  loss: 1.8546  time: 5.2209  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 750/3029]  eta: 3:18:27  lr: 0.000005  loss: 2.2284  time: 5.2641  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 800/3029]  eta: 3:14:06  lr: 0.000005  loss: 1.5107  time: 5.2283  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 850/3029]  eta: 3:09:44  lr: 0.000005  loss: 1.6548  time: 5.2079  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 900/3029]  eta: 3:05:21  lr: 0.000005  loss: 1.9328  time: 5.2064  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 950/3029]  eta: 3:01:00  lr: 0.000005  loss: 1.7542  time: 5.2301  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1000/3029]  eta: 2:56:39  lr: 0.000005  loss: 1.9878  time: 5.2303  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1050/3029]  eta: 2:52:17  lr: 0.000005  loss: 2.0518  time: 5.2369  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1100/3029]  eta: 2:47:56  lr: 0.000005  loss: 1.8973  time: 5.2256  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1150/3029]  eta: 2:43:34  lr: 0.000005  loss: 2.0067  time: 5.2174  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1200/3029]  eta: 2:39:14  lr: 0.000005  loss: 1.7771  time: 5.2368  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1250/3029]  eta: 2:34:51  lr: 0.000005  loss: 1.8546  time: 5.1940  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1300/3029]  eta: 2:30:30  lr: 0.000005  loss: 1.9716  time: 5.2259  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1350/3029]  eta: 2:26:08  lr: 0.000005  loss: 1.6660  time: 5.2195  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1400/3029]  eta: 2:21:47  lr: 0.000005  loss: 1.9536  time: 5.2206  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1450/3029]  eta: 2:17:26  lr: 0.000005  loss: 1.8751  time: 5.2316  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1500/3029]  eta: 2:13:05  lr: 0.000005  loss: 2.2130  time: 5.2206  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1550/3029]  eta: 2:08:44  lr: 0.000005  loss: 1.7024  time: 5.2200  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1600/3029]  eta: 2:04:23  lr: 0.000005  loss: 1.7744  time: 5.2262  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1650/3029]  eta: 2:00:02  lr: 0.000005  loss: 2.0498  time: 5.2260  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1700/3029]  eta: 1:55:41  lr: 0.000005  loss: 1.8514  time: 5.2252  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1750/3029]  eta: 1:51:20  lr: 0.000005  loss: 1.9464  time: 5.2098  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1800/3029]  eta: 1:46:59  lr: 0.000005  loss: 2.0890  time: 5.2202  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1850/3029]  eta: 1:42:38  lr: 0.000005  loss: 2.1193  time: 5.2405  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1900/3029]  eta: 1:38:17  lr: 0.000005  loss: 2.2746  time: 5.2095  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1950/3029]  eta: 1:33:56  lr: 0.000005  loss: 1.8605  time: 5.2244  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2000/3029]  eta: 1:29:35  lr: 0.000005  loss: 2.0696  time: 5.2175  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2050/3029]  eta: 1:25:13  lr: 0.000005  loss: 1.7898  time: 5.2137  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2100/3029]  eta: 1:20:52  lr: 0.000005  loss: 1.9455  time: 5.2171  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2150/3029]  eta: 1:16:31  lr: 0.000005  loss: 2.0515  time: 5.2131  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2200/3029]  eta: 1:12:09  lr: 0.000005  loss: 1.8065  time: 5.2047  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2250/3029]  eta: 1:07:48  lr: 0.000005  loss: 1.9421  time: 5.2298  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2300/3029]  eta: 1:03:27  lr: 0.000005  loss: 2.1611  time: 5.2179  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2350/3029]  eta: 0:59:06  lr: 0.000005  loss: 1.9526  time: 5.2093  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2400/3029]  eta: 0:54:44  lr: 0.000005  loss: 2.1524  time: 5.2234  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2450/3029]  eta: 0:50:24  lr: 0.000005  loss: 2.0420  time: 5.2486  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2500/3029]  eta: 0:46:02  lr: 0.000005  loss: 1.9976  time: 5.2360  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2550/3029]  eta: 0:41:41  lr: 0.000005  loss: 2.3339  time: 5.1963  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2600/3029]  eta: 0:37:20  lr: 0.000005  loss: 1.9701  time: 5.2333  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2650/3029]  eta: 0:32:59  lr: 0.000005  loss: 1.8695  time: 5.2395  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2700/3029]  eta: 0:28:38  lr: 0.000005  loss: 2.1165  time: 5.2145  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2750/3029]  eta: 0:24:17  lr: 0.000005  loss: 2.4169  time: 5.2210  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2800/3029]  eta: 0:19:55  lr: 0.000005  loss: 1.5561  time: 5.2327  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2850/3029]  eta: 0:15:34  lr: 0.000005  loss: 1.5867  time: 5.1982  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2900/3029]  eta: 0:11:13  lr: 0.000005  loss: 1.9528  time: 5.2334  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2950/3029]  eta: 0:06:52  lr: 0.000005  loss: 2.1954  time: 5.2209  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [3000/3029]  eta: 0:02:31  lr: 0.000005  loss: 2.0231  time: 5.1888  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [3028/3029]  eta: 0:00:05  lr: 0.000005  loss: 2.1401  time: 5.2138  data: 0.0000  max mem: 14580
Train: data epoch: [1] Total time: 4:23:38 (5.2222 s / it)
2023-08-22 00:22:16,598 [INFO] Averaged stats: lr: 0.0000  loss: 2.0046
2023-08-22 00:22:16,644 [INFO] No validation splits found.
2023-08-22 00:22:16,692 [INFO] Saving checkpoint at epoch 1 to /public/home/mswanghao/TorchProject/lavis/lavis/output_vqaDRSL/BLIP2/DRSL3_6_0_6/20230821152/checkpoint_1.pth.
2023-08-22 00:22:20,426 [INFO] No validation splits found.
2023-08-22 00:22:20,426 [INFO] Training time 8:49:19
