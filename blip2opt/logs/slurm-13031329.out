WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
loss DRSL3 b=1e-06 start=0 end=10loss DRSL3 b=1e-06 start=0 end=10

loss DRSL3 b=1e-06 start=0 end=10
loss DRSL3 b=1e-06 start=0 end=10
| distributed init (rank 2, world 4): env://
| distributed init (rank 1, world 4): env://| distributed init (rank 0, world 4): env://| distributed init (rank 3, world 4): env://


[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
2023-08-21 15:24:42,587 [INFO] 
=====  Running Parameters    =====
2023-08-21 15:24:42,588 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 32,
    "batch_size_train": 12,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "inference_method": "generate",
    "init_lr": 1e-05,
    "lr_layer_decay": 0.95,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 2,
    "max_len": 10,
    "min_len": 1,
    "min_lr": 0,
    "num_beams": 5,
    "num_workers": 4,
    "output_dir": "output_vqaDRSL/BLIP2/DRSL3_6_0_10",
    "prompt": "Question: {} Short answer:",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "vqa",
    "train_splits": [
        "train"
    ],
    "warmup_lr": 1e-08,
    "warmup_steps": 1000,
    "weight_decay": 0.05,
    "world_size": 4
}
2023-08-21 15:24:42,588 [INFO] 
======  Dataset Attributes  ======
2023-08-21 15:24:42,588 [INFO] 
======== vg_vqa =======
2023-08-21 15:24:42,589 [INFO] {
    "build_info": {
        "annotations": {
            "train": {
                "storage": "vg/annotations/vg_qa.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/visual_genome/vg_qa.json"
            }
        },
        "images": {
            "storage": "vg/images/"
        }
    },
    "data_type": "images",
    "text_processor": {
        "train": {
            "name": "blip_question"
        }
    },
    "vis_processor": {
        "train": {
            "image_size": 400,
            "name": "blip_image_train"
        }
    }
}
2023-08-21 15:24:42,589 [INFO] 
======  Model Attributes  ======
2023-08-21 15:24:42,590 [INFO] {
    "arch": "blip2_opt",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 400,
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "pretrain_opt2.7b",
    "num_query_token": 32,
    "opt_model": "facebook/opt-2.7b",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth",
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_model": "eva_clip_g",
    "vit_precision": "fp32"
}
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/vg/annotations/vg_qa.json
2023-08-21 15:24:42,639 [INFO] Building datasets...
BlipQuestionProcessor
Position interpolate from 16x16 to 28x28
2023-08-21 15:25:21,067 [INFO] freeze vision encoder
pretrain_path: https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-21 15:28:43,281 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-21 15:28:43,315 [INFO] Start training
2023-08-21 15:29:03,515 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-08-21 15:29:03,523 [INFO] Loaded 145395 records for train split from the dataset.
2023-08-21 15:29:03,586 [INFO] number of trainable parameters: 107133696
2023-08-21 15:29:03,587 [INFO] Start training epoch 0, 3029 iters per inner epoch.
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
Train: data epoch: [0]  [   0/3029]  eta: 21:50:30  lr: 0.000000  loss: 2.7844  time: 25.9591  data: 0.0000  max mem: 12766
2023-08-21 15:29:29,597 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  50/3029]  eta: 4:38:43  lr: 0.000001  loss: 2.6774  time: 5.1888  data: 0.0000  max mem: 14218
Train: data epoch: [0]  [ 100/3029]  eta: 4:24:30  lr: 0.000001  loss: 2.5713  time: 5.2278  data: 0.0000  max mem: 14218
Train: data epoch: [0]  [ 150/3029]  eta: 4:16:52  lr: 0.000002  loss: 2.5212  time: 5.2209  data: 0.0000  max mem: 14288
Train: data epoch: [0]  [ 200/3029]  eta: 4:11:01  lr: 0.000002  loss: 1.7822  time: 5.2052  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 250/3029]  eta: 4:05:45  lr: 0.000003  loss: 1.8246  time: 5.2469  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 300/3029]  eta: 4:00:41  lr: 0.000003  loss: 2.2237  time: 5.2023  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 350/3029]  eta: 3:56:03  lr: 0.000004  loss: 2.0645  time: 5.2972  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 400/3029]  eta: 3:51:36  lr: 0.000004  loss: 2.3628  time: 5.2440  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 450/3029]  eta: 3:47:11  lr: 0.000005  loss: 2.0184  time: 5.3012  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 500/3029]  eta: 3:42:52  lr: 0.000005  loss: 2.0536  time: 5.3006  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 550/3029]  eta: 3:38:23  lr: 0.000006  loss: 1.5386  time: 5.2508  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 600/3029]  eta: 3:33:53  lr: 0.000006  loss: 2.1509  time: 5.2782  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 650/3029]  eta: 3:29:29  lr: 0.000007  loss: 2.4667  time: 5.2979  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 700/3029]  eta: 3:25:05  lr: 0.000007  loss: 1.8719  time: 5.2733  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 750/3029]  eta: 3:20:41  lr: 0.000008  loss: 2.2310  time: 5.3106  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 800/3029]  eta: 3:16:17  lr: 0.000008  loss: 1.8590  time: 5.2942  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 850/3029]  eta: 3:11:54  lr: 0.000009  loss: 2.3047  time: 5.3069  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 900/3029]  eta: 3:07:31  lr: 0.000009  loss: 2.2508  time: 5.2858  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 950/3029]  eta: 3:03:07  lr: 0.000010  loss: 2.2863  time: 5.2884  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1000/3029]  eta: 2:58:42  lr: 0.000010  loss: 1.7507  time: 5.2715  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1050/3029]  eta: 2:54:18  lr: 0.000010  loss: 2.0294  time: 5.2990  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1100/3029]  eta: 2:49:54  lr: 0.000010  loss: 2.1781  time: 5.2757  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1150/3029]  eta: 2:45:30  lr: 0.000010  loss: 2.0662  time: 5.2812  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1200/3029]  eta: 2:41:06  lr: 0.000010  loss: 2.0372  time: 5.2578  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1250/3029]  eta: 2:36:42  lr: 0.000010  loss: 2.2752  time: 5.2923  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1300/3029]  eta: 2:32:18  lr: 0.000010  loss: 2.0378  time: 5.2797  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1350/3029]  eta: 2:27:52  lr: 0.000010  loss: 1.8611  time: 5.2597  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1400/3029]  eta: 2:23:28  lr: 0.000010  loss: 1.9183  time: 5.2880  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1450/3029]  eta: 2:19:04  lr: 0.000010  loss: 1.9848  time: 5.2871  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1500/3029]  eta: 2:14:40  lr: 0.000010  loss: 2.1088  time: 5.2862  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1550/3029]  eta: 2:10:17  lr: 0.000010  loss: 1.9177  time: 5.3209  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1600/3029]  eta: 2:05:53  lr: 0.000010  loss: 2.1459  time: 5.2869  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1650/3029]  eta: 2:01:28  lr: 0.000010  loss: 1.9718  time: 5.2851  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1700/3029]  eta: 1:57:03  lr: 0.000010  loss: 2.4299  time: 5.2060  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1750/3029]  eta: 1:52:36  lr: 0.000010  loss: 2.0050  time: 5.2234  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1800/3029]  eta: 1:48:10  lr: 0.000010  loss: 1.7780  time: 5.2375  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1850/3029]  eta: 1:43:45  lr: 0.000010  loss: 1.9021  time: 5.2390  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1900/3029]  eta: 1:39:19  lr: 0.000010  loss: 2.2867  time: 5.2281  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1950/3029]  eta: 1:34:54  lr: 0.000010  loss: 1.8640  time: 5.2333  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2000/3029]  eta: 1:30:28  lr: 0.000010  loss: 1.6867  time: 5.2263  data: 0.0007  max mem: 14487
Train: data epoch: [0]  [2050/3029]  eta: 1:26:03  lr: 0.000010  loss: 2.1488  time: 5.2048  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2100/3029]  eta: 1:21:38  lr: 0.000010  loss: 2.1150  time: 5.2197  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2150/3029]  eta: 1:17:13  lr: 0.000010  loss: 2.3201  time: 5.2075  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2200/3029]  eta: 1:12:49  lr: 0.000010  loss: 2.2391  time: 5.2068  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2250/3029]  eta: 1:08:24  lr: 0.000010  loss: 1.9543  time: 5.2324  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2300/3029]  eta: 1:04:00  lr: 0.000010  loss: 2.4532  time: 5.2248  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2350/3029]  eta: 0:59:36  lr: 0.000010  loss: 2.2945  time: 5.2275  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2400/3029]  eta: 0:55:12  lr: 0.000010  loss: 2.1348  time: 5.2138  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2450/3029]  eta: 0:50:48  lr: 0.000010  loss: 1.8393  time: 5.2112  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2500/3029]  eta: 0:46:25  lr: 0.000010  loss: 2.0609  time: 5.2434  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2550/3029]  eta: 0:42:01  lr: 0.000010  loss: 2.1269  time: 5.2502  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2600/3029]  eta: 0:37:38  lr: 0.000010  loss: 2.1619  time: 5.2302  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2650/3029]  eta: 0:33:14  lr: 0.000010  loss: 1.8424  time: 5.2325  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2700/3029]  eta: 0:28:51  lr: 0.000010  loss: 2.2037  time: 5.2473  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2750/3029]  eta: 0:24:27  lr: 0.000010  loss: 1.8150  time: 5.2173  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2800/3029]  eta: 0:20:04  lr: 0.000010  loss: 1.8190  time: 5.2104  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2850/3029]  eta: 0:15:41  lr: 0.000010  loss: 1.7006  time: 5.2039  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2900/3029]  eta: 0:11:18  lr: 0.000010  loss: 1.7763  time: 5.2440  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2950/3029]  eta: 0:06:55  lr: 0.000010  loss: 2.2605  time: 5.2149  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [3000/3029]  eta: 0:02:32  lr: 0.000010  loss: 2.0473  time: 5.2274  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [3028/3029]  eta: 0:00:05  lr: 0.000010  loss: 1.9284  time: 5.1948  data: 0.0000  max mem: 14563
Train: data epoch: [0] Total time: 4:25:25 (5.2578 s / it)
2023-08-21 19:54:29,364 [INFO] Averaged stats: lr: 0.0000  loss: 2.0502
2023-08-21 19:54:29,430 [INFO] No validation splits found.
2023-08-21 19:54:29,481 [INFO] Saving checkpoint at epoch 0 to /public/home/mswanghao/TorchProject/lavis/lavis/output_vqaDRSL/BLIP2/DRSL3_6_0_10/20230821152/checkpoint_0.pth.
2023-08-21 19:54:35,119 [INFO] Start training
2023-08-21 19:54:35,165 [INFO] Start training epoch 1, 3029 iters per inner epoch.
Train: data epoch: [1]  [   0/3029]  eta: 9:29:12  lr: 0.000005  loss: 1.8531  time: 11.2753  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [  50/3029]  eta: 4:24:45  lr: 0.000005  loss: 1.7703  time: 5.2004  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 100/3029]  eta: 4:17:42  lr: 0.000005  loss: 1.7360  time: 5.2494  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 150/3029]  eta: 4:12:29  lr: 0.000005  loss: 2.4658  time: 5.2450  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 200/3029]  eta: 4:07:39  lr: 0.000005  loss: 2.1274  time: 5.2277  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 250/3029]  eta: 4:03:02  lr: 0.000005  loss: 1.7290  time: 5.2380  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 300/3029]  eta: 3:58:26  lr: 0.000005  loss: 2.0304  time: 5.2185  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 350/3029]  eta: 3:53:53  lr: 0.000005  loss: 1.8565  time: 5.2099  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 400/3029]  eta: 3:49:28  lr: 0.000005  loss: 2.0202  time: 5.2356  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 450/3029]  eta: 3:44:59  lr: 0.000005  loss: 1.7203  time: 5.2205  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 500/3029]  eta: 3:40:35  lr: 0.000005  loss: 2.1506  time: 5.2498  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 550/3029]  eta: 3:36:10  lr: 0.000005  loss: 1.9928  time: 5.2154  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 600/3029]  eta: 3:31:44  lr: 0.000005  loss: 2.3023  time: 5.2058  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 650/3029]  eta: 3:27:21  lr: 0.000005  loss: 1.9612  time: 5.2177  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 700/3029]  eta: 3:22:58  lr: 0.000005  loss: 1.8658  time: 5.2273  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 750/3029]  eta: 3:18:33  lr: 0.000005  loss: 2.2416  time: 5.1764  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 800/3029]  eta: 3:14:12  lr: 0.000005  loss: 1.4916  time: 5.2254  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 850/3029]  eta: 3:09:49  lr: 0.000005  loss: 1.6655  time: 5.2274  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 900/3029]  eta: 3:05:29  lr: 0.000005  loss: 1.9265  time: 5.2301  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 950/3029]  eta: 3:01:09  lr: 0.000005  loss: 1.7507  time: 5.2302  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1000/3029]  eta: 2:56:47  lr: 0.000005  loss: 2.0079  time: 5.2249  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1050/3029]  eta: 2:52:26  lr: 0.000005  loss: 2.0349  time: 5.2363  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1100/3029]  eta: 2:48:06  lr: 0.000005  loss: 1.8993  time: 5.2367  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1150/3029]  eta: 2:43:44  lr: 0.000005  loss: 2.0299  time: 5.2277  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1200/3029]  eta: 2:39:21  lr: 0.000005  loss: 1.7825  time: 5.2251  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1250/3029]  eta: 2:34:59  lr: 0.000005  loss: 1.8654  time: 5.2043  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1300/3029]  eta: 2:30:36  lr: 0.000005  loss: 1.9622  time: 5.1934  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1350/3029]  eta: 2:26:14  lr: 0.000005  loss: 1.6495  time: 5.2111  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1400/3029]  eta: 2:21:52  lr: 0.000005  loss: 1.9354  time: 5.2056  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1450/3029]  eta: 2:17:31  lr: 0.000005  loss: 1.8819  time: 5.2439  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1500/3029]  eta: 2:13:10  lr: 0.000005  loss: 2.2108  time: 5.2349  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1550/3029]  eta: 2:08:47  lr: 0.000005  loss: 1.6979  time: 5.2041  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1600/3029]  eta: 2:04:26  lr: 0.000005  loss: 1.7874  time: 5.2163  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1650/3029]  eta: 2:00:05  lr: 0.000005  loss: 2.0478  time: 5.2352  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1700/3029]  eta: 1:55:44  lr: 0.000005  loss: 1.8579  time: 5.2334  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1750/3029]  eta: 1:51:22  lr: 0.000005  loss: 1.9457  time: 5.2097  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1800/3029]  eta: 1:47:01  lr: 0.000005  loss: 2.0901  time: 5.2377  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1850/3029]  eta: 1:42:40  lr: 0.000005  loss: 2.1233  time: 5.2340  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1900/3029]  eta: 1:38:19  lr: 0.000005  loss: 2.2508  time: 5.2109  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1950/3029]  eta: 1:33:58  lr: 0.000005  loss: 1.8530  time: 5.2197  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2000/3029]  eta: 1:29:36  lr: 0.000005  loss: 2.0860  time: 5.2081  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2050/3029]  eta: 1:25:15  lr: 0.000005  loss: 1.8044  time: 5.2139  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2100/3029]  eta: 1:20:53  lr: 0.000005  loss: 1.9391  time: 5.2081  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2150/3029]  eta: 1:16:32  lr: 0.000005  loss: 2.0703  time: 5.2216  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2200/3029]  eta: 1:12:11  lr: 0.000005  loss: 1.8149  time: 5.2100  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2250/3029]  eta: 1:07:50  lr: 0.000005  loss: 1.9465  time: 5.2348  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2300/3029]  eta: 1:03:28  lr: 0.000005  loss: 2.1488  time: 5.2127  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2350/3029]  eta: 0:59:07  lr: 0.000005  loss: 1.9583  time: 5.2141  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2400/3029]  eta: 0:54:46  lr: 0.000005  loss: 2.1539  time: 5.2450  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2450/3029]  eta: 0:50:24  lr: 0.000005  loss: 2.0478  time: 5.2442  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2500/3029]  eta: 0:46:03  lr: 0.000005  loss: 2.0052  time: 5.2281  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2550/3029]  eta: 0:41:42  lr: 0.000005  loss: 2.3327  time: 5.2281  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2600/3029]  eta: 0:37:21  lr: 0.000005  loss: 1.9512  time: 5.2244  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2650/3029]  eta: 0:32:59  lr: 0.000005  loss: 1.8471  time: 5.2236  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2700/3029]  eta: 0:28:38  lr: 0.000005  loss: 2.1224  time: 5.2114  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2750/3029]  eta: 0:24:17  lr: 0.000005  loss: 2.4137  time: 5.2279  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2800/3029]  eta: 0:19:56  lr: 0.000005  loss: 1.5699  time: 5.2297  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2850/3029]  eta: 0:15:35  lr: 0.000005  loss: 1.6040  time: 5.1895  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2900/3029]  eta: 0:11:13  lr: 0.000005  loss: 1.9734  time: 5.1981  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2950/3029]  eta: 0:06:52  lr: 0.000005  loss: 2.1971  time: 5.2306  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [3000/3029]  eta: 0:02:31  lr: 0.000005  loss: 2.0303  time: 5.2095  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [3028/3029]  eta: 0:00:05  lr: 0.000005  loss: 2.1446  time: 5.2226  data: 0.0000  max mem: 14580
Train: data epoch: [1] Total time: 4:23:42 (5.2237 s / it)
2023-08-22 00:18:17,923 [INFO] Averaged stats: lr: 0.0000  loss: 2.0043
2023-08-22 00:18:17,974 [INFO] No validation splits found.
2023-08-22 00:18:18,046 [INFO] Saving checkpoint at epoch 1 to /public/home/mswanghao/TorchProject/lavis/lavis/output_vqaDRSL/BLIP2/DRSL3_6_0_10/20230821152/checkpoint_1.pth.
2023-08-22 00:18:24,213 [INFO] No validation splits found.
2023-08-22 00:18:24,230 [INFO] Training time 8:49:40
