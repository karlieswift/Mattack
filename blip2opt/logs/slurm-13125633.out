WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
loss DRSL3 b=0.0001 start=0 end=100loss DRSL3 b=0.0001 start=0 end=100

loss DRSL3 b=0.0001 start=0 end=100loss DRSL3 b=0.0001 start=0 end=100

| distributed init (rank 2, world 4): env://
| distributed init (rank 1, world 4): env://
| distributed init (rank 0, world 4): env://| distributed init (rank 3, world 4): env://

[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
2023-08-23 14:52:28,166 [INFO] 
=====  Running Parameters    =====
2023-08-23 14:52:28,166 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 2,
    "batch_size_train": 12,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "inference_method": "generate",
    "init_lr": 1e-05,
    "lr_layer_decay": 0.95,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 5,
    "max_len": 10,
    "min_len": 1,
    "min_lr": 0,
    "num_beams": 5,
    "num_workers": 4,
    "output_dir": "output_vqacsu/BLIP2/DRSL3_4_0_100",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "vqa",
    "train_splits": [
        "train"
    ],
    "warmup_lr": 1e-08,
    "warmup_steps": 1000,
    "weight_decay": 0.05,
    "world_size": 4
}
2023-08-23 14:52:28,167 [INFO] 
======  Dataset Attributes  ======
2023-08-23 14:52:28,167 [INFO] 
======== vg_vqa =======
2023-08-23 14:52:28,167 [INFO] {
    "build_info": {
        "annotations": {
            "train": {
                "storage": "vg/annotations/vg_qa.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/visual_genome/vg_qa.json"
            }
        },
        "images": {
            "storage": "vg/images/"
        }
    },
    "data_type": "images",
    "text_processor": {
        "train": {
            "name": "blip_question"
        }
    },
    "vis_processor": {
        "train": {
            "image_size": 400,
            "name": "blip_image_train"
        }
    }
}
2023-08-23 14:52:28,167 [INFO] 
======  Model Attributes  ======
2023-08-23 14:52:28,168 [INFO] {
    "arch": "blip2_opt",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 400,
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "pretrain_opt2.7b",
    "num_query_token": 32,
    "opt_model": "facebook/opt-2.7b",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth",
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_model": "eva_clip_g",
    "vit_precision": "fp32"
}
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/vg/annotations/vg_qa.json
2023-08-23 14:52:28,183 [INFO] Building datasets...
BlipQuestionProcessor
Position interpolate from 16x16 to 28x28
2023-08-23 14:53:07,072 [INFO] freeze vision encoder
pretrain_path: https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-23 14:56:31,748 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-23 14:56:31,771 [INFO] Start training
2023-08-23 14:56:58,359 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-08-23 14:56:58,421 [INFO] Loaded 13756 records for train split from the dataset.
2023-08-23 14:56:58,517 [INFO] number of trainable parameters: 107133696
2023-08-23 14:56:58,518 [INFO] Start training epoch 0, 286 iters per inner epoch.
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
Train: data epoch: [0]  [  0/286]  eta: 1:59:52  lr: 0.000000  loss: 3.0435  time: 25.1487  data: 0.0000  max mem: 12837
2023-08-23 14:57:23,687 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [ 50/286]  eta: 0:22:05  lr: 0.000001  loss: 3.1066  time: 5.2054  data: 0.0000  max mem: 14125
Train: data epoch: [0]  [100/286]  eta: 0:16:47  lr: 0.000001  loss: 2.8964  time: 5.2027  data: 0.0000  max mem: 14132
Train: data epoch: [0]  [150/286]  eta: 0:12:08  lr: 0.000002  loss: 2.5118  time: 5.2284  data: 0.0000  max mem: 14132
Train: data epoch: [0]  [200/286]  eta: 0:07:37  lr: 0.000002  loss: 1.5958  time: 5.2085  data: 0.0000  max mem: 14132
Train: data epoch: [0]  [250/286]  eta: 0:03:10  lr: 0.000003  loss: 1.1200  time: 5.2314  data: 0.0000  max mem: 14132
Train: data epoch: [0]  [285/286]  eta: 0:00:05  lr: 0.000003  loss: 0.6597  time: 5.2265  data: 0.0000  max mem: 14132
Train: data epoch: [0] Total time: 0:25:12 (5.2901 s / it)
2023-08-23 15:22:11,508 [INFO] Averaged stats: lr: 0.0000  loss: 2.2259
2023-08-23 15:22:11,526 [INFO] No validation splits found.
2023-08-23 15:22:11,587 [INFO] Saving checkpoint at epoch 0 to /public/home/mswanghao/TorchProject/lavis/lavis/output_vqacsu/BLIP2/DRSL3_4_0_100/20230823145/checkpoint_0.pth.
2023-08-23 15:22:16,211 [INFO] Start training
2023-08-23 15:22:16,256 [INFO] Start training epoch 1, 286 iters per inner epoch.
Train: data epoch: [1]  [  0/286]  eta: 0:46:56  lr: 0.000009  loss: 0.6052  time: 9.8468  data: 0.0000  max mem: 14132
Train: data epoch: [1]  [ 50/286]  eta: 0:20:52  lr: 0.000009  loss: 0.5057  time: 5.2048  data: 0.0000  max mem: 14132
Train: data epoch: [1]  [100/286]  eta: 0:16:19  lr: 0.000009  loss: 0.5606  time: 5.2270  data: 0.0000  max mem: 14144
Train: data epoch: [1]  [150/286]  eta: 0:11:52  lr: 0.000009  loss: 0.6891  time: 5.2065  data: 0.0000  max mem: 14144
Train: data epoch: [1]  [200/286]  eta: 0:07:30  lr: 0.000009  loss: 0.7900  time: 5.2157  data: 0.0000  max mem: 14144
Train: data epoch: [1]  [250/286]  eta: 0:03:08  lr: 0.000009  loss: 0.6757  time: 5.2241  data: 0.0000  max mem: 14144
Train: data epoch: [1]  [285/286]  eta: 0:00:05  lr: 0.000009  loss: 0.4628  time: 5.2011  data: 0.0000  max mem: 14144
Train: data epoch: [1] Total time: 0:24:55 (5.2298 s / it)
2023-08-23 15:47:12,000 [INFO] Averaged stats: lr: 0.0000  loss: 0.6069
2023-08-23 15:47:12,047 [INFO] No validation splits found.
2023-08-23 15:47:12,102 [INFO] Saving checkpoint at epoch 1 to /public/home/mswanghao/TorchProject/lavis/lavis/output_vqacsu/BLIP2/DRSL3_4_0_100/20230823145/checkpoint_1.pth.
2023-08-23 15:47:17,362 [INFO] Start training
2023-08-23 15:47:17,409 [INFO] Start training epoch 2, 286 iters per inner epoch.
Train: data epoch: [2]  [  0/286]  eta: 0:46:52  lr: 0.000007  loss: 0.5135  time: 9.8336  data: 0.0000  max mem: 14144
Train: data epoch: [2]  [ 50/286]  eta: 0:20:52  lr: 0.000007  loss: 0.4686  time: 5.2189  data: 0.0000  max mem: 14144
Train: data epoch: [2]  [100/286]  eta: 0:16:18  lr: 0.000007  loss: 0.6035  time: 5.2098  data: 0.0000  max mem: 14144
Train: data epoch: [2]  [150/286]  eta: 0:11:53  lr: 0.000007  loss: 0.8170  time: 5.2222  data: 0.0000  max mem: 14144
Train: data epoch: [2]  [200/286]  eta: 0:07:30  lr: 0.000007  loss: 0.5233  time: 5.2199  data: 0.0000  max mem: 14144
Train: data epoch: [2]  [250/286]  eta: 0:03:08  lr: 0.000007  loss: 0.7245  time: 5.2062  data: 0.0000  max mem: 14144
slurmstepd: error: *** JOB 13125633 ON b08r2n13 CANCELLED AT 2023-08-23T16:09:14 ***
