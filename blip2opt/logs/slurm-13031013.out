WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
loss DRSL3 b=0.0001 start=0 end=10loss DRSL3 b=0.0001 start=0 end=10

loss DRSL3 b=0.0001 start=0 end=10loss DRSL3 b=0.0001 start=0 end=10

| distributed init (rank 0, world 4): env://| distributed init (rank 2, world 4): env://| distributed init (rank 1, world 4): env://| distributed init (rank 3, world 4): env://



[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
2023-08-21 15:03:22,129 [INFO] 
=====  Running Parameters    =====
2023-08-21 15:03:22,130 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 32,
    "batch_size_train": 12,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "inference_method": "generate",
    "init_lr": 1e-05,
    "lr_layer_decay": 0.95,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 2,
    "max_len": 10,
    "min_len": 1,
    "min_lr": 0,
    "num_beams": 5,
    "num_workers": 4,
    "output_dir": "output_vqaDRSL/BLIP2/DRSL3_4_0_10",
    "prompt": "Question: {} Short answer:",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "vqa",
    "train_splits": [
        "train"
    ],
    "warmup_lr": 1e-08,
    "warmup_steps": 1000,
    "weight_decay": 0.05,
    "world_size": 4
}
2023-08-21 15:03:22,130 [INFO] 
======  Dataset Attributes  ======
2023-08-21 15:03:22,130 [INFO] 
======== vg_vqa =======
2023-08-21 15:03:22,131 [INFO] {
    "build_info": {
        "annotations": {
            "train": {
                "storage": "vg/annotations/vg_qa.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/visual_genome/vg_qa.json"
            }
        },
        "images": {
            "storage": "vg/images/"
        }
    },
    "data_type": "images",
    "text_processor": {
        "train": {
            "name": "blip_question"
        }
    },
    "vis_processor": {
        "train": {
            "image_size": 400,
            "name": "blip_image_train"
        }
    }
}
2023-08-21 15:03:22,131 [INFO] 
======  Model Attributes  ======
2023-08-21 15:03:22,132 [INFO] {
    "arch": "blip2_opt",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 400,
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "pretrain_opt2.7b",
    "num_query_token": 32,
    "opt_model": "facebook/opt-2.7b",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth",
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_model": "eva_clip_g",
    "vit_precision": "fp32"
}
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/vg/annotations/vg_qa.json
2023-08-21 15:03:22,138 [INFO] Building datasets...
BlipQuestionProcessor
Position interpolate from 16x16 to 28x28
2023-08-21 15:04:00,067 [INFO] freeze vision encoder
pretrain_path: https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-21 15:07:22,692 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-21 15:07:22,726 [INFO] Start training
2023-08-21 15:07:43,869 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-08-21 15:07:43,870 [INFO] Loaded 145395 records for train split from the dataset.
2023-08-21 15:07:43,971 [INFO] number of trainable parameters: 107133696
2023-08-21 15:07:43,973 [INFO] Start training epoch 0, 3029 iters per inner epoch.
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
Train: data epoch: [0]  [   0/3029]  eta: 21:55:03  lr: 0.000000  loss: 2.8434  time: 26.0495  data: 0.0000  max mem: 12766
2023-08-21 15:08:10,122 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  50/3029]  eta: 4:39:47  lr: 0.000001  loss: 2.7404  time: 5.1965  data: 0.0000  max mem: 14218
Train: data epoch: [0]  [ 100/3029]  eta: 4:25:08  lr: 0.000001  loss: 2.6323  time: 5.2394  data: 0.0000  max mem: 14218
Train: data epoch: [0]  [ 150/3029]  eta: 4:17:17  lr: 0.000002  loss: 2.5919  time: 5.2142  data: 0.0000  max mem: 14288
Train: data epoch: [0]  [ 200/3029]  eta: 4:11:16  lr: 0.000002  loss: 1.8450  time: 5.2326  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 250/3029]  eta: 4:05:51  lr: 0.000003  loss: 1.8934  time: 5.2308  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 300/3029]  eta: 4:00:41  lr: 0.000003  loss: 2.3028  time: 5.2076  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 350/3029]  eta: 3:55:58  lr: 0.000004  loss: 2.1335  time: 5.2662  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 400/3029]  eta: 3:51:35  lr: 0.000004  loss: 2.4373  time: 5.2919  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 450/3029]  eta: 3:47:09  lr: 0.000005  loss: 2.0693  time: 5.2630  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 500/3029]  eta: 3:42:48  lr: 0.000005  loss: 2.1573  time: 5.2682  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 550/3029]  eta: 3:38:20  lr: 0.000006  loss: 1.6139  time: 5.2809  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 600/3029]  eta: 3:33:57  lr: 0.000006  loss: 2.2147  time: 5.3203  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 650/3029]  eta: 3:29:33  lr: 0.000007  loss: 2.5061  time: 5.2943  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 700/3029]  eta: 3:25:08  lr: 0.000007  loss: 1.9636  time: 5.3000  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 750/3029]  eta: 3:20:45  lr: 0.000008  loss: 2.3373  time: 5.3038  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 800/3029]  eta: 3:16:22  lr: 0.000008  loss: 1.9240  time: 5.2848  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 850/3029]  eta: 3:12:00  lr: 0.000009  loss: 2.3708  time: 5.3241  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 900/3029]  eta: 3:07:38  lr: 0.000009  loss: 2.3009  time: 5.3282  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 950/3029]  eta: 3:03:12  lr: 0.000010  loss: 2.3598  time: 5.2970  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1000/3029]  eta: 2:58:47  lr: 0.000010  loss: 1.7918  time: 5.2619  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1050/3029]  eta: 2:54:21  lr: 0.000010  loss: 2.0642  time: 5.2891  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1100/3029]  eta: 2:49:54  lr: 0.000010  loss: 2.2283  time: 5.2542  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1150/3029]  eta: 2:45:30  lr: 0.000010  loss: 2.1269  time: 5.2576  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1200/3029]  eta: 2:41:08  lr: 0.000010  loss: 2.0912  time: 5.3146  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1250/3029]  eta: 2:36:44  lr: 0.000010  loss: 2.3651  time: 5.3194  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1300/3029]  eta: 2:32:18  lr: 0.000010  loss: 2.1124  time: 5.2649  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1350/3029]  eta: 2:27:52  lr: 0.000010  loss: 1.9192  time: 5.2425  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1400/3029]  eta: 2:23:28  lr: 0.000010  loss: 1.9911  time: 5.2775  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1450/3029]  eta: 2:19:03  lr: 0.000010  loss: 2.0567  time: 5.2811  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1500/3029]  eta: 2:14:39  lr: 0.000010  loss: 2.1881  time: 5.2841  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1550/3029]  eta: 2:10:15  lr: 0.000010  loss: 1.9762  time: 5.2807  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1600/3029]  eta: 2:05:51  lr: 0.000010  loss: 2.2038  time: 5.3194  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1650/3029]  eta: 2:01:26  lr: 0.000010  loss: 2.0225  time: 5.2913  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1700/3029]  eta: 1:57:01  lr: 0.000010  loss: 2.5462  time: 5.2157  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1750/3029]  eta: 1:52:34  lr: 0.000010  loss: 2.0319  time: 5.2315  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1800/3029]  eta: 1:48:08  lr: 0.000010  loss: 1.8999  time: 5.2226  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1850/3029]  eta: 1:43:43  lr: 0.000010  loss: 1.9879  time: 5.2154  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1900/3029]  eta: 1:39:17  lr: 0.000010  loss: 2.3721  time: 5.2444  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1950/3029]  eta: 1:34:52  lr: 0.000010  loss: 1.9407  time: 5.2165  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2000/3029]  eta: 1:30:27  lr: 0.000010  loss: 1.7574  time: 5.2229  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2050/3029]  eta: 1:26:02  lr: 0.000010  loss: 2.2187  time: 5.2241  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2100/3029]  eta: 1:21:37  lr: 0.000010  loss: 2.2010  time: 5.2128  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2150/3029]  eta: 1:17:12  lr: 0.000010  loss: 2.3903  time: 5.2113  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2200/3029]  eta: 1:12:48  lr: 0.000010  loss: 2.2830  time: 5.2283  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2250/3029]  eta: 1:08:23  lr: 0.000010  loss: 2.0166  time: 5.2268  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2300/3029]  eta: 1:04:00  lr: 0.000010  loss: 2.4925  time: 5.2409  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2350/3029]  eta: 0:59:36  lr: 0.000010  loss: 2.3495  time: 5.2541  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2400/3029]  eta: 0:55:12  lr: 0.000010  loss: 2.1820  time: 5.2304  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2450/3029]  eta: 0:50:48  lr: 0.000010  loss: 1.9048  time: 5.2287  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2500/3029]  eta: 0:46:24  lr: 0.000010  loss: 2.1061  time: 5.2285  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2550/3029]  eta: 0:42:01  lr: 0.000010  loss: 2.1395  time: 5.2698  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2600/3029]  eta: 0:37:37  lr: 0.000010  loss: 2.2239  time: 5.2220  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2650/3029]  eta: 0:33:14  lr: 0.000010  loss: 1.9190  time: 5.2389  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2700/3029]  eta: 0:28:51  lr: 0.000010  loss: 2.2700  time: 5.2215  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2750/3029]  eta: 0:24:27  lr: 0.000010  loss: 1.8841  time: 5.2372  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2800/3029]  eta: 0:20:04  lr: 0.000010  loss: 1.8807  time: 5.2298  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2850/3029]  eta: 0:15:41  lr: 0.000010  loss: 1.7680  time: 5.2339  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2900/3029]  eta: 0:11:18  lr: 0.000010  loss: 1.8194  time: 5.2076  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2950/3029]  eta: 0:06:55  lr: 0.000010  loss: 2.3309  time: 5.2039  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [3000/3029]  eta: 0:02:32  lr: 0.000010  loss: 2.1161  time: 5.2294  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [3028/3029]  eta: 0:00:05  lr: 0.000010  loss: 1.9933  time: 5.1994  data: 0.0000  max mem: 14563
Train: data epoch: [0] Total time: 4:25:25 (5.2577 s / it)
2023-08-21 19:33:09,519 [INFO] Averaged stats: lr: 0.0000  loss: 2.1183
2023-08-21 19:33:09,571 [INFO] No validation splits found.
2023-08-21 19:33:09,642 [INFO] Saving checkpoint at epoch 0 to /public/home/mswanghao/TorchProject/lavis/lavis/output_vqaDRSL/BLIP2/DRSL3_4_0_10/20230821150/checkpoint_0.pth.
2023-08-21 19:33:15,344 [INFO] Start training
2023-08-21 19:33:15,435 [INFO] Start training epoch 1, 3029 iters per inner epoch.
Train: data epoch: [1]  [   0/3029]  eta: 9:34:17  lr: 0.000005  loss: 1.9226  time: 11.3760  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [  50/3029]  eta: 4:25:04  lr: 0.000005  loss: 1.8239  time: 5.2114  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 100/3029]  eta: 4:18:02  lr: 0.000005  loss: 1.8053  time: 5.2495  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 150/3029]  eta: 4:12:38  lr: 0.000005  loss: 2.5729  time: 5.2165  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 200/3029]  eta: 4:07:56  lr: 0.000005  loss: 2.1837  time: 5.2541  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 250/3029]  eta: 4:03:12  lr: 0.000005  loss: 1.7587  time: 5.2385  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 300/3029]  eta: 3:58:34  lr: 0.000005  loss: 2.0760  time: 5.2126  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 350/3029]  eta: 3:54:03  lr: 0.000005  loss: 1.9604  time: 5.2260  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 400/3029]  eta: 3:49:37  lr: 0.000005  loss: 2.0828  time: 5.2293  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 450/3029]  eta: 3:45:10  lr: 0.000005  loss: 1.8038  time: 5.2110  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 500/3029]  eta: 3:40:44  lr: 0.000005  loss: 2.1968  time: 5.2158  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 550/3029]  eta: 3:36:17  lr: 0.000005  loss: 2.0640  time: 5.2079  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 600/3029]  eta: 3:31:48  lr: 0.000005  loss: 2.3585  time: 5.1943  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 650/3029]  eta: 3:27:26  lr: 0.000005  loss: 1.9957  time: 5.2267  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 700/3029]  eta: 3:23:04  lr: 0.000005  loss: 1.9306  time: 5.2168  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 750/3029]  eta: 3:18:38  lr: 0.000005  loss: 2.2804  time: 5.1929  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 800/3029]  eta: 3:14:18  lr: 0.000005  loss: 1.6003  time: 5.2323  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 850/3029]  eta: 3:09:53  lr: 0.000005  loss: 1.7032  time: 5.1893  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 900/3029]  eta: 3:05:30  lr: 0.000005  loss: 2.0391  time: 5.2080  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 950/3029]  eta: 3:01:08  lr: 0.000005  loss: 1.8075  time: 5.2192  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1000/3029]  eta: 2:56:47  lr: 0.000005  loss: 2.1013  time: 5.2274  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1050/3029]  eta: 2:52:25  lr: 0.000005  loss: 2.0998  time: 5.2444  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1100/3029]  eta: 2:48:04  lr: 0.000005  loss: 1.9786  time: 5.2220  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1150/3029]  eta: 2:43:43  lr: 0.000005  loss: 2.0878  time: 5.2132  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1200/3029]  eta: 2:39:21  lr: 0.000005  loss: 1.8505  time: 5.2028  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1250/3029]  eta: 2:34:58  lr: 0.000005  loss: 1.9297  time: 5.1997  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1300/3029]  eta: 2:30:36  lr: 0.000005  loss: 2.0299  time: 5.2235  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1350/3029]  eta: 2:26:15  lr: 0.000005  loss: 1.7339  time: 5.2368  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1400/3029]  eta: 2:21:54  lr: 0.000005  loss: 2.0283  time: 5.2361  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1450/3029]  eta: 2:17:33  lr: 0.000005  loss: 1.9652  time: 5.2313  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1500/3029]  eta: 2:13:12  lr: 0.000005  loss: 2.2662  time: 5.2422  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1550/3029]  eta: 2:08:50  lr: 0.000005  loss: 1.7995  time: 5.2144  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1600/3029]  eta: 2:04:28  lr: 0.000005  loss: 1.8721  time: 5.2257  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1650/3029]  eta: 2:00:07  lr: 0.000005  loss: 2.1346  time: 5.2231  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1700/3029]  eta: 1:55:45  lr: 0.000005  loss: 1.9125  time: 5.1997  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1750/3029]  eta: 1:51:23  lr: 0.000005  loss: 2.0134  time: 5.2106  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1800/3029]  eta: 1:47:02  lr: 0.000005  loss: 2.1928  time: 5.2197  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1850/3029]  eta: 1:42:41  lr: 0.000005  loss: 2.1906  time: 5.2199  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1900/3029]  eta: 1:38:19  lr: 0.000005  loss: 2.3422  time: 5.2121  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1950/3029]  eta: 1:33:58  lr: 0.000005  loss: 1.9390  time: 5.2087  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2000/3029]  eta: 1:29:37  lr: 0.000005  loss: 2.1739  time: 5.2321  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2050/3029]  eta: 1:25:16  lr: 0.000005  loss: 1.8463  time: 5.2280  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2100/3029]  eta: 1:20:55  lr: 0.000005  loss: 2.0204  time: 5.2297  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2150/3029]  eta: 1:16:33  lr: 0.000005  loss: 2.1310  time: 5.2311  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2200/3029]  eta: 1:12:12  lr: 0.000005  loss: 1.8806  time: 5.2282  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2250/3029]  eta: 1:07:50  lr: 0.000005  loss: 2.0068  time: 5.2232  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2300/3029]  eta: 1:03:29  lr: 0.000005  loss: 2.2629  time: 5.2326  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2350/3029]  eta: 0:59:08  lr: 0.000005  loss: 2.0347  time: 5.1850  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2400/3029]  eta: 0:54:46  lr: 0.000005  loss: 2.2306  time: 5.2328  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2450/3029]  eta: 0:50:25  lr: 0.000005  loss: 2.0500  time: 5.2413  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2500/3029]  eta: 0:46:04  lr: 0.000005  loss: 2.0449  time: 5.2102  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2550/3029]  eta: 0:41:42  lr: 0.000005  loss: 2.3810  time: 5.2137  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2600/3029]  eta: 0:37:21  lr: 0.000005  loss: 2.0232  time: 5.2464  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2650/3029]  eta: 0:33:00  lr: 0.000005  loss: 1.9646  time: 5.2350  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2700/3029]  eta: 0:28:39  lr: 0.000005  loss: 2.1970  time: 5.2172  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2750/3029]  eta: 0:24:17  lr: 0.000005  loss: 2.4550  time: 5.2155  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2800/3029]  eta: 0:19:56  lr: 0.000005  loss: 1.6226  time: 5.2277  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2850/3029]  eta: 0:15:35  lr: 0.000005  loss: 1.6552  time: 5.2129  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2900/3029]  eta: 0:11:14  lr: 0.000005  loss: 2.0683  time: 5.2403  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2950/3029]  eta: 0:06:52  lr: 0.000005  loss: 2.2605  time: 5.2143  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [3000/3029]  eta: 0:02:31  lr: 0.000005  loss: 2.0795  time: 5.2054  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [3028/3029]  eta: 0:00:05  lr: 0.000005  loss: 2.2169  time: 5.2268  data: 0.0000  max mem: 14580
Train: data epoch: [1] Total time: 4:23:48 (5.2256 s / it)
2023-08-21 23:57:03,871 [INFO] Averaged stats: lr: 0.0000  loss: 2.0758
2023-08-21 23:57:03,903 [INFO] No validation splits found.
2023-08-21 23:57:03,964 [INFO] Saving checkpoint at epoch 1 to /public/home/mswanghao/TorchProject/lavis/lavis/output_vqaDRSL/BLIP2/DRSL3_4_0_10/20230821150/checkpoint_1.pth.
2023-08-21 23:57:08,578 [INFO] No validation splits found.
2023-08-21 23:57:08,581 [INFO] Training time 8:49:45
