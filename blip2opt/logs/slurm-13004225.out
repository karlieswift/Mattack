WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
loss DRSL3 b=1e-06 start=0 end=20loss DRSL3 b=1e-06 start=0 end=20

loss DRSL3 b=1e-06 start=0 end=20
loss DRSL3 b=1e-06 start=0 end=20
| distributed init (rank 3, world 4): env://
| distributed init (rank 0, world 4): env://| distributed init (rank 2, world 4): env://

| distributed init (rank 1, world 4): env://
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
2023-08-18 23:26:27,833 [INFO] 
=====  Running Parameters    =====
2023-08-18 23:26:27,834 [INFO] {
    "amp": true,
    "batch_size_eval": 2,
    "batch_size_train": 16,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 0.0005,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 10,
    "min_lr": 1e-05,
    "num_workers": 4,
    "output_dir": "output1/BLIP2/DRSL3_0_20_Pretrain_stage2",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "image_text_pretrain",
    "train_splits": [
        "train"
    ],
    "warmup_lr": 1e-06,
    "warmup_steps": 2000,
    "weight_decay": 0.05,
    "world_size": 4
}
2023-08-18 23:26:27,834 [INFO] 
======  Dataset Attributes  ======
2023-08-18 23:26:27,834 [INFO] 
======== coco_caption =======
2023-08-18 23:26:27,835 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "coco/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "coco/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "coco/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "coco/images/"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "train": {
            "name": "blip_caption"
        }
    },
    "vis_processor": {
        "train": {
            "image_size": 224,
            "name": "blip2_image_train"
        }
    }
}
2023-08-18 23:26:27,835 [INFO] 
======  Model Attributes  ======
2023-08-18 23:26:27,835 [INFO] {
    "arch": "blip2_opt",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 224,
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "pretrain_opt2.7b",
    "num_query_token": 32,
    "opt_model": "facebook/opt-2.7b",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained.pth",
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_precision": "fp16"
}
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/coco/annotations/coco_karpathy_train.json
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/coco/annotations/coco_karpathy_val.json
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/coco/annotations/coco_karpathy_test.json
2023-08-18 23:26:27,846 [INFO] Building datasets...
2023-08-18 23:27:11,081 [INFO] freeze vision encoder
2023-08-18 23:30:36,033 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained.pth
2023-08-18 23:30:36,081 [INFO] Start training
2023-08-18 23:30:52,714 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-08-18 23:30:52,726 [INFO] Loaded 566747 records for train split from the dataset.
2023-08-18 23:30:52,726 [INFO] Loaded 5000 records for val split from the dataset.
2023-08-18 23:30:52,726 [INFO] Loaded 5000 records for test split from the dataset.
2023-08-18 23:30:52,818 [INFO] number of trainable parameters: 107133696
2023-08-18 23:30:52,819 [INFO] Start training epoch 0, 8855 iters per inner epoch.
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
Train: data epoch: [0]  [   0/8855]  eta: 2 days, 12:49:16  lr: 0.000001  loss: 6.2723  time: 24.7269  data: 0.0000  max mem: 11493
2023-08-18 23:31:17,610 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  50/8855]  eta: 11:14:53  lr: 0.000013  loss: 4.3017  time: 4.1770  data: 0.0000  max mem: 13571
Train: data epoch: [0]  [ 100/8855]  eta: 10:31:32  lr: 0.000026  loss: 3.8198  time: 4.0833  data: 0.0000  max mem: 13571
Train: data epoch: [0]  [ 150/8855]  eta: 10:13:56  lr: 0.000038  loss: 3.4722  time: 4.0364  data: 0.0000  max mem: 13571
Train: data epoch: [0]  [ 200/8855]  eta: 10:03:32  lr: 0.000051  loss: 2.7147  time: 4.1183  data: 0.0000  max mem: 13571
Train: data epoch: [0]  [ 250/8855]  eta: 9:56:52  lr: 0.000063  loss: 3.3428  time: 4.1065  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 300/8855]  eta: 9:51:24  lr: 0.000076  loss: 2.5972  time: 4.1051  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 350/8855]  eta: 9:46:11  lr: 0.000088  loss: 2.5568  time: 4.0569  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 400/8855]  eta: 9:41:15  lr: 0.000101  loss: 2.4361  time: 4.0488  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 450/8855]  eta: 9:38:37  lr: 0.000113  loss: 2.2522  time: 4.1349  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 500/8855]  eta: 9:34:28  lr: 0.000126  loss: 2.4079  time: 4.0229  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 550/8855]  eta: 9:30:38  lr: 0.000138  loss: 2.4733  time: 4.0347  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 600/8855]  eta: 9:26:43  lr: 0.000151  loss: 2.4077  time: 4.0534  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 650/8855]  eta: 9:22:32  lr: 0.000163  loss: 2.2190  time: 4.0674  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 700/8855]  eta: 9:18:11  lr: 0.000176  loss: 2.4692  time: 4.0243  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 750/8855]  eta: 9:13:54  lr: 0.000188  loss: 2.4110  time: 4.0452  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 800/8855]  eta: 9:10:30  lr: 0.000201  loss: 2.1450  time: 4.1141  data: 0.0000  max mem: 13573
Train: data epoch: [0]  [ 850/8855]  eta: 9:06:49  lr: 0.000213  loss: 2.5594  time: 4.0729  data: 0.0000  max mem: 13595
Train: data epoch: [0]  [ 900/8855]  eta: 9:03:25  lr: 0.000226  loss: 2.3597  time: 4.0949  data: 0.0000  max mem: 13595
Train: data epoch: [0]  [ 950/8855]  eta: 8:59:44  lr: 0.000238  loss: 2.2667  time: 4.0685  data: 0.0000  max mem: 13595
Train: data epoch: [0]  [1000/8855]  eta: 8:56:05  lr: 0.000251  loss: 2.1568  time: 4.0531  data: 0.0000  max mem: 13595
Train: data epoch: [0]  [1050/8855]  eta: 8:52:28  lr: 0.000263  loss: 2.3672  time: 4.0579  data: 0.0000  max mem: 13595
Train: data epoch: [0]  [1100/8855]  eta: 8:49:16  lr: 0.000275  loss: 2.6880  time: 4.0726  data: 0.0000  max mem: 13595
Train: data epoch: [0]  [1150/8855]  eta: 8:45:35  lr: 0.000288  loss: 2.1477  time: 4.0488  data: 0.0000  max mem: 13595
Train: data epoch: [0]  [1200/8855]  eta: 8:42:19  lr: 0.000300  loss: 2.7449  time: 4.1210  data: 0.0000  max mem: 13595
Train: data epoch: [0]  [1250/8855]  eta: 8:38:41  lr: 0.000313  loss: 2.2549  time: 4.0906  data: 0.0000  max mem: 13595
Train: data epoch: [0]  [1300/8855]  eta: 8:35:10  lr: 0.000325  loss: 2.8055  time: 4.0815  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1350/8855]  eta: 8:31:28  lr: 0.000338  loss: 2.6740  time: 4.0075  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1400/8855]  eta: 8:28:10  lr: 0.000350  loss: 2.2620  time: 4.1571  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1450/8855]  eta: 8:24:49  lr: 0.000363  loss: 2.8428  time: 4.0749  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1500/8855]  eta: 8:21:26  lr: 0.000375  loss: 2.5526  time: 4.1180  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1550/8855]  eta: 8:17:44  lr: 0.000388  loss: 2.3680  time: 4.0193  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1600/8855]  eta: 8:14:37  lr: 0.000400  loss: 2.9222  time: 4.3031  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1650/8855]  eta: 8:12:18  lr: 0.000413  loss: 2.3637  time: 4.3589  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1700/8855]  eta: 8:09:36  lr: 0.000425  loss: 2.0949  time: 4.3507  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1750/8855]  eta: 8:06:38  lr: 0.000438  loss: 2.3402  time: 4.2082  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1800/8855]  eta: 8:03:57  lr: 0.000450  loss: 2.3243  time: 4.3928  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1850/8855]  eta: 8:01:20  lr: 0.000463  loss: 2.4616  time: 4.3379  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1900/8855]  eta: 7:58:38  lr: 0.000475  loss: 2.4561  time: 4.3657  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [1950/8855]  eta: 7:55:54  lr: 0.000488  loss: 2.3275  time: 4.3801  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2000/8855]  eta: 7:53:00  lr: 0.000500  loss: 2.3934  time: 4.3473  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2050/8855]  eta: 7:49:48  lr: 0.000500  loss: 2.5355  time: 4.2041  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2100/8855]  eta: 7:46:59  lr: 0.000500  loss: 2.4270  time: 4.4095  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2150/8855]  eta: 7:44:01  lr: 0.000500  loss: 2.2507  time: 4.3273  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2200/8855]  eta: 7:40:45  lr: 0.000500  loss: 2.1495  time: 4.2505  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2250/8855]  eta: 7:37:23  lr: 0.000500  loss: 2.0154  time: 4.2207  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2300/8855]  eta: 7:34:07  lr: 0.000500  loss: 2.4559  time: 4.2898  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2350/8855]  eta: 7:30:50  lr: 0.000500  loss: 2.1633  time: 4.2559  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2400/8855]  eta: 7:27:49  lr: 0.000500  loss: 2.4491  time: 4.3989  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2450/8855]  eta: 7:24:51  lr: 0.000500  loss: 2.1878  time: 4.3670  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2500/8855]  eta: 7:21:44  lr: 0.000500  loss: 2.1959  time: 4.3503  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2550/8855]  eta: 7:18:42  lr: 0.000500  loss: 2.4824  time: 4.3619  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2600/8855]  eta: 7:15:32  lr: 0.000500  loss: 1.8579  time: 4.3046  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2650/8855]  eta: 7:12:31  lr: 0.000500  loss: 2.4926  time: 4.3583  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2700/8855]  eta: 7:09:19  lr: 0.000500  loss: 2.4232  time: 4.3463  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2750/8855]  eta: 7:06:06  lr: 0.000500  loss: 2.1010  time: 4.2374  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2800/8855]  eta: 7:02:48  lr: 0.000500  loss: 2.3637  time: 4.2760  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2850/8855]  eta: 6:59:39  lr: 0.000500  loss: 2.0614  time: 4.4027  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2900/8855]  eta: 6:56:21  lr: 0.000500  loss: 2.3808  time: 4.3236  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [2950/8855]  eta: 6:53:07  lr: 0.000500  loss: 1.9872  time: 4.4016  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3000/8855]  eta: 6:49:39  lr: 0.000500  loss: 2.4182  time: 4.0968  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3050/8855]  eta: 6:45:53  lr: 0.000500  loss: 2.6086  time: 4.0161  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3100/8855]  eta: 6:42:11  lr: 0.000500  loss: 2.3663  time: 4.0654  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3150/8855]  eta: 6:38:30  lr: 0.000500  loss: 2.1452  time: 4.0395  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3200/8855]  eta: 6:34:49  lr: 0.000500  loss: 2.2130  time: 4.0794  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3250/8855]  eta: 6:31:11  lr: 0.000500  loss: 2.0030  time: 4.1078  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3300/8855]  eta: 6:27:30  lr: 0.000500  loss: 2.3754  time: 4.0445  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3350/8855]  eta: 6:23:47  lr: 0.000500  loss: 2.1113  time: 4.0093  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3400/8855]  eta: 6:20:10  lr: 0.000500  loss: 2.0687  time: 4.0765  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3450/8855]  eta: 6:16:39  lr: 0.000500  loss: 2.4115  time: 4.1898  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3500/8855]  eta: 6:13:01  lr: 0.000500  loss: 2.2590  time: 4.0309  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3550/8855]  eta: 6:09:27  lr: 0.000500  loss: 2.2318  time: 4.0904  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3600/8855]  eta: 6:05:52  lr: 0.000500  loss: 2.5991  time: 4.0934  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3650/8855]  eta: 6:02:15  lr: 0.000500  loss: 2.4417  time: 4.0805  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3700/8855]  eta: 5:58:41  lr: 0.000500  loss: 2.4004  time: 4.1100  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3750/8855]  eta: 5:55:09  lr: 0.000500  loss: 2.3952  time: 4.1179  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3800/8855]  eta: 5:51:36  lr: 0.000500  loss: 2.5237  time: 4.1163  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3850/8855]  eta: 5:48:00  lr: 0.000500  loss: 2.4102  time: 4.0767  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3900/8855]  eta: 5:44:27  lr: 0.000500  loss: 2.0381  time: 4.0803  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [3950/8855]  eta: 5:40:50  lr: 0.000500  loss: 2.1465  time: 4.0444  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4000/8855]  eta: 5:37:17  lr: 0.000500  loss: 2.4089  time: 4.1077  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4050/8855]  eta: 5:33:43  lr: 0.000500  loss: 2.4636  time: 4.0612  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4100/8855]  eta: 5:30:12  lr: 0.000500  loss: 2.5498  time: 4.1605  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4150/8855]  eta: 5:26:40  lr: 0.000500  loss: 2.2048  time: 4.0746  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4200/8855]  eta: 5:23:05  lr: 0.000500  loss: 2.0656  time: 4.0350  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4250/8855]  eta: 5:19:33  lr: 0.000500  loss: 2.2346  time: 4.1747  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4300/8855]  eta: 5:16:02  lr: 0.000500  loss: 2.0579  time: 4.1697  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4350/8855]  eta: 5:12:28  lr: 0.000500  loss: 2.2266  time: 4.0358  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4400/8855]  eta: 5:08:55  lr: 0.000500  loss: 2.3227  time: 4.0041  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4450/8855]  eta: 5:05:26  lr: 0.000500  loss: 2.0501  time: 4.1850  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4500/8855]  eta: 5:01:52  lr: 0.000500  loss: 2.7230  time: 4.0376  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4550/8855]  eta: 4:58:20  lr: 0.000500  loss: 2.3502  time: 4.0760  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4600/8855]  eta: 4:54:47  lr: 0.000500  loss: 2.3929  time: 4.1257  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4650/8855]  eta: 4:51:15  lr: 0.000500  loss: 2.1744  time: 4.0056  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4700/8855]  eta: 4:47:45  lr: 0.000500  loss: 2.1505  time: 4.0838  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4750/8855]  eta: 4:44:16  lr: 0.000500  loss: 2.6026  time: 4.1130  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4800/8855]  eta: 4:40:44  lr: 0.000500  loss: 2.2446  time: 4.0795  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4850/8855]  eta: 4:37:13  lr: 0.000500  loss: 2.0380  time: 4.0808  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4900/8855]  eta: 4:33:42  lr: 0.000500  loss: 2.4672  time: 4.0571  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [4950/8855]  eta: 4:30:12  lr: 0.000500  loss: 2.1602  time: 4.0882  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5000/8855]  eta: 4:26:41  lr: 0.000500  loss: 1.9809  time: 4.0491  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5050/8855]  eta: 4:23:11  lr: 0.000500  loss: 2.4373  time: 4.1347  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5100/8855]  eta: 4:19:40  lr: 0.000500  loss: 2.0934  time: 4.0391  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5150/8855]  eta: 4:16:12  lr: 0.000500  loss: 2.4533  time: 4.1123  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5200/8855]  eta: 4:12:41  lr: 0.000500  loss: 2.1829  time: 4.0348  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5250/8855]  eta: 4:09:12  lr: 0.000500  loss: 2.0600  time: 4.0008  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5300/8855]  eta: 4:05:44  lr: 0.000500  loss: 2.1988  time: 4.1238  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5350/8855]  eta: 4:02:14  lr: 0.000500  loss: 2.0753  time: 4.0884  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5400/8855]  eta: 3:58:44  lr: 0.000500  loss: 2.0459  time: 4.0184  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5450/8855]  eta: 3:55:15  lr: 0.000500  loss: 2.1780  time: 4.0611  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5500/8855]  eta: 3:51:46  lr: 0.000500  loss: 2.1289  time: 4.0682  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5550/8855]  eta: 3:48:16  lr: 0.000500  loss: 1.9519  time: 4.0973  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5600/8855]  eta: 3:44:46  lr: 0.000500  loss: 1.9820  time: 4.0655  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5650/8855]  eta: 3:41:17  lr: 0.000500  loss: 2.1045  time: 4.0330  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5700/8855]  eta: 3:37:47  lr: 0.000500  loss: 2.3815  time: 4.0445  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5750/8855]  eta: 3:34:18  lr: 0.000500  loss: 2.3088  time: 4.0756  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5800/8855]  eta: 3:30:51  lr: 0.000500  loss: 1.8408  time: 4.0629  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5850/8855]  eta: 3:27:21  lr: 0.000500  loss: 2.2948  time: 4.0544  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5900/8855]  eta: 3:23:53  lr: 0.000500  loss: 1.7789  time: 4.1002  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [5950/8855]  eta: 3:20:25  lr: 0.000500  loss: 2.6168  time: 4.1331  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6000/8855]  eta: 3:16:56  lr: 0.000500  loss: 2.0887  time: 4.1049  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6050/8855]  eta: 3:13:30  lr: 0.000500  loss: 2.4279  time: 4.2798  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6100/8855]  eta: 3:10:03  lr: 0.000500  loss: 2.1500  time: 4.0718  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6150/8855]  eta: 3:06:35  lr: 0.000500  loss: 2.3780  time: 4.1056  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6200/8855]  eta: 3:03:06  lr: 0.000500  loss: 2.4162  time: 4.0234  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6250/8855]  eta: 2:59:38  lr: 0.000500  loss: 2.2108  time: 4.0857  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6300/8855]  eta: 2:56:11  lr: 0.000500  loss: 2.0675  time: 4.1475  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6350/8855]  eta: 2:52:44  lr: 0.000500  loss: 2.2278  time: 4.1672  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6400/8855]  eta: 2:49:17  lr: 0.000500  loss: 2.3838  time: 4.1129  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6450/8855]  eta: 2:45:49  lr: 0.000500  loss: 2.3963  time: 4.0458  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6500/8855]  eta: 2:42:22  lr: 0.000500  loss: 2.2179  time: 4.1114  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6550/8855]  eta: 2:38:55  lr: 0.000500  loss: 2.6022  time: 4.1868  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6600/8855]  eta: 2:35:27  lr: 0.000500  loss: 2.1396  time: 4.1897  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6650/8855]  eta: 2:32:01  lr: 0.000500  loss: 2.2267  time: 4.1756  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6700/8855]  eta: 2:28:33  lr: 0.000500  loss: 2.1358  time: 4.0915  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6750/8855]  eta: 2:25:07  lr: 0.000500  loss: 2.5074  time: 4.1593  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6800/8855]  eta: 2:21:39  lr: 0.000500  loss: 2.1164  time: 4.0424  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6850/8855]  eta: 2:18:12  lr: 0.000500  loss: 1.9570  time: 4.0751  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6900/8855]  eta: 2:14:44  lr: 0.000500  loss: 1.9912  time: 4.0403  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [6950/8855]  eta: 2:11:17  lr: 0.000500  loss: 2.2499  time: 4.1100  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7000/8855]  eta: 2:07:50  lr: 0.000500  loss: 2.1187  time: 4.1324  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7050/8855]  eta: 2:04:23  lr: 0.000500  loss: 2.3051  time: 4.0844  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7100/8855]  eta: 2:00:56  lr: 0.000500  loss: 2.1639  time: 4.0670  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7150/8855]  eta: 1:57:28  lr: 0.000500  loss: 2.5676  time: 4.0648  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7200/8855]  eta: 1:54:01  lr: 0.000500  loss: 2.0221  time: 4.1264  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7250/8855]  eta: 1:50:35  lr: 0.000500  loss: 2.1569  time: 4.2146  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7300/8855]  eta: 1:47:09  lr: 0.000500  loss: 2.3262  time: 4.0702  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7350/8855]  eta: 1:43:42  lr: 0.000500  loss: 2.4838  time: 4.1065  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7400/8855]  eta: 1:40:15  lr: 0.000500  loss: 2.4326  time: 4.0488  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7450/8855]  eta: 1:36:47  lr: 0.000500  loss: 2.4101  time: 4.0502  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7500/8855]  eta: 1:33:20  lr: 0.000500  loss: 2.4484  time: 4.0985  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7550/8855]  eta: 1:29:54  lr: 0.000500  loss: 2.4569  time: 4.1029  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7600/8855]  eta: 1:26:27  lr: 0.000500  loss: 2.5233  time: 4.0942  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7650/8855]  eta: 1:22:59  lr: 0.000500  loss: 2.2149  time: 4.1222  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7700/8855]  eta: 1:19:32  lr: 0.000500  loss: 2.3961  time: 4.0893  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7750/8855]  eta: 1:16:06  lr: 0.000500  loss: 2.0418  time: 4.1676  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7800/8855]  eta: 1:12:39  lr: 0.000500  loss: 2.2831  time: 4.0264  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7850/8855]  eta: 1:09:12  lr: 0.000500  loss: 2.1611  time: 4.0241  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7900/8855]  eta: 1:05:45  lr: 0.000500  loss: 2.1307  time: 4.1270  data: 0.0000  max mem: 13602
Train: data epoch: [0]  [7950/8855]  eta: 1:02:18  lr: 0.000500  loss: 2.4568  time: 4.0934  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8000/8855]  eta: 0:58:51  lr: 0.000500  loss: 2.2043  time: 4.0562  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8050/8855]  eta: 0:55:25  lr: 0.000500  loss: 2.5056  time: 4.0934  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8100/8855]  eta: 0:51:58  lr: 0.000500  loss: 2.3530  time: 4.0686  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8150/8855]  eta: 0:48:31  lr: 0.000500  loss: 2.0029  time: 4.0927  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8200/8855]  eta: 0:45:05  lr: 0.000500  loss: 2.2161  time: 4.1187  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8250/8855]  eta: 0:41:38  lr: 0.000500  loss: 2.1055  time: 4.0930  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8300/8855]  eta: 0:38:12  lr: 0.000500  loss: 2.1465  time: 4.1589  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8350/8855]  eta: 0:34:45  lr: 0.000500  loss: 2.4512  time: 4.0556  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8400/8855]  eta: 0:31:18  lr: 0.000500  loss: 2.0094  time: 4.1302  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8450/8855]  eta: 0:27:52  lr: 0.000500  loss: 2.0919  time: 4.0946  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8500/8855]  eta: 0:24:25  lr: 0.000500  loss: 2.3705  time: 4.1274  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8550/8855]  eta: 0:20:59  lr: 0.000500  loss: 2.3965  time: 4.0651  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8600/8855]  eta: 0:17:32  lr: 0.000500  loss: 2.0044  time: 4.1798  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8650/8855]  eta: 0:14:06  lr: 0.000500  loss: 2.3135  time: 4.1699  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8700/8855]  eta: 0:10:39  lr: 0.000500  loss: 2.3641  time: 4.0973  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8750/8855]  eta: 0:07:13  lr: 0.000500  loss: 2.1955  time: 4.0681  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8800/8855]  eta: 0:03:47  lr: 0.000500  loss: 2.1126  time: 4.0256  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8850/8855]  eta: 0:00:20  lr: 0.000500  loss: 1.9851  time: 4.0984  data: 0.0000  max mem: 13616
Train: data epoch: [0]  [8854/8855]  eta: 0:00:04  lr: 0.000500  loss: 2.1646  time: 4.1350  data: 0.0000  max mem: 13616
Train: data epoch: [0] Total time: 10:09:12 (4.1279 s / it)
2023-08-19 09:40:05,813 [INFO] Averaged stats: lr: 0.0004  loss: 2.3337
2023-08-19 09:40:05,883 [INFO] No validation splits found.
2023-08-19 09:40:05,935 [INFO] Saving checkpoint at epoch 0 to /public/home/mswanghao/TorchProject/lavis/lavis/output1/BLIP2/DRSL3_0_20_Pretrain_stage2/20230818232/checkpoint_0.pth.
2023-08-19 09:40:09,707 [INFO] Start training
2023-08-19 09:40:09,788 [INFO] Start training epoch 1, 8855 iters per inner epoch.
Train: data epoch: [1]  [   0/8855]  eta: 20:22:19  lr: 0.000488  loss: 2.0453  time: 8.2823  data: 0.0000  max mem: 13616
Train: data epoch: [1]  [  50/8855]  eta: 10:20:21  lr: 0.000488  loss: 2.0685  time: 4.1486  data: 0.0000  max mem: 13616
Train: data epoch: [1]  [ 100/8855]  eta: 10:14:52  lr: 0.000488  loss: 2.3212  time: 4.2477  data: 0.0000  max mem: 13616
Train: data epoch: [1]  [ 150/8855]  eta: 10:08:43  lr: 0.000488  loss: 2.3901  time: 4.1697  data: 0.0000  max mem: 13616
Train: data epoch: [1]  [ 200/8855]  eta: 10:00:59  lr: 0.000488  loss: 2.2085  time: 4.0414  data: 0.0000  max mem: 13616
slurmstepd: error: *** JOB 13004225 ON a13r2n11 CANCELLED AT 2023-08-19T09:55:32 ***
