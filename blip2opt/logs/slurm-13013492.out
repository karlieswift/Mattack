WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
loss CE b=1e-05 start=0 end=100loss CE b=1e-05 start=0 end=100

loss CE b=1e-05 start=0 end=100
loss CE b=1e-05 start=0 end=100
| distributed init (rank 3, world 4): env://| distributed init (rank 2, world 4): env://| distributed init (rank 1, world 4): env://| distributed init (rank 0, world 4): env://



[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
2023-08-19 20:53:20,406 [INFO] 
=====  Running Parameters    =====
2023-08-19 20:53:20,407 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 32,
    "batch_size_train": 16,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "inference_method": "generate",
    "init_lr": 1e-05,
    "lr_layer_decay": 0.95,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 2,
    "max_len": 10,
    "min_len": 1,
    "min_lr": 0,
    "num_beams": 5,
    "num_workers": 4,
    "output_dir": "outputvqa/BLIP2/CE",
    "prompt": "Question: {} Short answer:",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "vqa",
    "train_splits": [
        "train"
    ],
    "warmup_lr": 1e-08,
    "warmup_steps": 1000,
    "weight_decay": 0.05,
    "world_size": 4
}
2023-08-19 20:53:20,407 [INFO] 
======  Dataset Attributes  ======
2023-08-19 20:53:20,407 [INFO] 
======== vg_vqa =======
2023-08-19 20:53:20,408 [INFO] {
    "build_info": {
        "annotations": {
            "train": {
                "storage": "vg/annotations/vg_qa.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/visual_genome/vg_qa.json"
            }
        },
        "images": {
            "storage": "vg/images/"
        }
    },
    "data_type": "images",
    "text_processor": {
        "train": {
            "name": "blip_question"
        }
    },
    "vis_processor": {
        "train": {
            "image_size": 400,
            "name": "blip_image_train"
        }
    }
}
2023-08-19 20:53:20,408 [INFO] 
======  Model Attributes  ======
2023-08-19 20:53:20,408 [INFO] {
    "arch": "blip2_opt",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 400,
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "pretrain_opt2.7b",
    "num_query_token": 32,
    "opt_model": "facebook/opt-2.7b",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth",
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_model": "eva_clip_g",
    "vit_precision": "fp32"
}
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/vg/annotations/vg_qa.json
2023-08-19 20:53:20,423 [INFO] Building datasets...
BlipQuestionProcessor
Position interpolate from 16x16 to 28x28
2023-08-19 20:53:58,335 [INFO] freeze vision encoder
https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-19 20:57:20,903 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-19 20:57:20,928 [INFO] Start training
2023-08-19 20:57:47,356 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-08-19 20:57:47,357 [INFO] Loaded 145395 records for train split from the dataset.
2023-08-19 20:57:47,446 [INFO] number of trainable parameters: 107133696
2023-08-19 20:57:47,447 [INFO] Start training epoch 0, 2271 iters per inner epoch.
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
Train: data epoch: [0]  [   0/2271]  eta: 16:12:36  lr: 0.000000  loss: 2.7285  time: 25.6963  data: 0.0002  max mem: 13328
2023-08-19 20:58:13,151 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  50/2271]  eta: 4:10:34  lr: 0.000001  loss: 2.7912  time: 6.3927  data: 0.0000  max mem: 14825
Train: data epoch: [0]  [ 100/2271]  eta: 3:58:09  lr: 0.000001  loss: 2.3674  time: 6.3908  data: 0.0000  max mem: 14928
Train: data epoch: [0]  [ 150/2271]  eta: 3:50:51  lr: 0.000002  loss: 2.3358  time: 6.4062  data: 0.0000  max mem: 15039
Train: data epoch: [0]  [ 200/2271]  eta: 3:44:24  lr: 0.000002  loss: 2.4242  time: 6.4233  data: 0.0000  max mem: 15039
Train: data epoch: [0]  [ 250/2271]  eta: 3:38:22  lr: 0.000003  loss: 1.8515  time: 6.4200  data: 0.0000  max mem: 15039
Train: data epoch: [0]  [ 300/2271]  eta: 3:32:35  lr: 0.000003  loss: 2.2094  time: 6.4039  data: 0.0000  max mem: 15039
Train: data epoch: [0]  [ 350/2271]  eta: 3:26:57  lr: 0.000004  loss: 2.1420  time: 6.4079  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 400/2271]  eta: 3:21:23  lr: 0.000004  loss: 2.2350  time: 6.4035  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 450/2271]  eta: 3:15:53  lr: 0.000005  loss: 2.1162  time: 6.4113  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 500/2271]  eta: 3:10:24  lr: 0.000005  loss: 1.8632  time: 6.4337  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 550/2271]  eta: 3:04:54  lr: 0.000006  loss: 1.9315  time: 6.3925  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 600/2271]  eta: 2:59:30  lr: 0.000006  loss: 1.9207  time: 6.4289  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 650/2271]  eta: 2:54:04  lr: 0.000007  loss: 2.0552  time: 6.3961  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 700/2271]  eta: 2:48:39  lr: 0.000007  loss: 1.9979  time: 6.4014  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 750/2271]  eta: 2:43:14  lr: 0.000008  loss: 1.7584  time: 6.4207  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 800/2271]  eta: 2:37:49  lr: 0.000008  loss: 2.1985  time: 6.4065  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 850/2271]  eta: 2:32:24  lr: 0.000009  loss: 1.9179  time: 6.3995  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 900/2271]  eta: 2:27:00  lr: 0.000009  loss: 2.2671  time: 6.4100  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [ 950/2271]  eta: 2:21:37  lr: 0.000010  loss: 1.9962  time: 6.4267  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [1000/2271]  eta: 2:16:14  lr: 0.000010  loss: 2.1357  time: 6.4265  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [1050/2271]  eta: 2:10:50  lr: 0.000010  loss: 2.0418  time: 6.3914  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [1100/2271]  eta: 2:05:28  lr: 0.000010  loss: 1.9974  time: 6.4425  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [1150/2271]  eta: 2:00:05  lr: 0.000010  loss: 1.9985  time: 6.3881  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [1200/2271]  eta: 1:54:43  lr: 0.000010  loss: 2.2823  time: 6.4249  data: 0.0000  max mem: 15095
Train: data epoch: [0]  [1250/2271]  eta: 1:49:21  lr: 0.000010  loss: 1.9016  time: 6.4098  data: 0.0000  max mem: 15148
Train: data epoch: [0]  [1300/2271]  eta: 1:43:58  lr: 0.000010  loss: 2.4209  time: 6.3674  data: 0.0000  max mem: 15148
Train: data epoch: [0]  [1350/2271]  eta: 1:38:36  lr: 0.000010  loss: 1.8464  time: 6.4080  data: 0.0000  max mem: 15148
Train: data epoch: [0]  [1400/2271]  eta: 1:33:13  lr: 0.000010  loss: 2.0936  time: 6.3580  data: 0.0000  max mem: 15148
Train: data epoch: [0]  [1450/2271]  eta: 1:27:51  lr: 0.000010  loss: 1.9034  time: 6.3551  data: 0.0000  max mem: 15148
Train: data epoch: [0]  [1500/2271]  eta: 1:22:28  lr: 0.000010  loss: 1.8421  time: 6.3809  data: 0.0000  max mem: 15148
Train: data epoch: [0]  [1550/2271]  eta: 1:17:06  lr: 0.000010  loss: 2.0170  time: 6.3482  data: 0.0000  max mem: 15148
Train: data epoch: [0]  [1600/2271]  eta: 1:11:44  lr: 0.000010  loss: 1.8979  time: 6.3795  data: 0.0000  max mem: 15148
Train: data epoch: [0]  [1650/2271]  eta: 1:06:23  lr: 0.000010  loss: 2.2291  time: 6.3748  data: 0.0000  max mem: 15148
Train: data epoch: [0]  [1700/2271]  eta: 1:01:01  lr: 0.000010  loss: 2.0002  time: 6.3809  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [1750/2271]  eta: 0:55:40  lr: 0.000010  loss: 2.0457  time: 6.3924  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [1800/2271]  eta: 0:50:19  lr: 0.000010  loss: 1.9743  time: 6.3839  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [1850/2271]  eta: 0:44:58  lr: 0.000010  loss: 1.7128  time: 6.3357  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [1900/2271]  eta: 0:39:37  lr: 0.000010  loss: 2.0032  time: 6.4193  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [1950/2271]  eta: 0:34:17  lr: 0.000010  loss: 2.1238  time: 6.4105  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [2000/2271]  eta: 0:28:56  lr: 0.000010  loss: 2.1171  time: 6.3600  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [2050/2271]  eta: 0:23:36  lr: 0.000010  loss: 1.8374  time: 6.3850  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [2100/2271]  eta: 0:18:15  lr: 0.000010  loss: 1.7334  time: 6.3958  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [2150/2271]  eta: 0:12:55  lr: 0.000010  loss: 2.0802  time: 6.3930  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [2200/2271]  eta: 0:07:34  lr: 0.000010  loss: 2.3374  time: 6.3690  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [2250/2271]  eta: 0:02:14  lr: 0.000010  loss: 2.0855  time: 6.3484  data: 0.0000  max mem: 15204
Train: data epoch: [0]  [2270/2271]  eta: 0:00:06  lr: 0.000010  loss: 2.0827  time: 6.3349  data: 0.0000  max mem: 15204
Train: data epoch: [0] Total time: 4:02:24 (6.4044 s / it)
2023-08-20 01:00:11,860 [INFO] Averaged stats: lr: 0.0000  loss: 2.0593
2023-08-20 01:00:11,909 [INFO] No validation splits found.
2023-08-20 01:00:11,960 [INFO] Saving checkpoint at epoch 0 to /public/home/mswanghao/TorchProject/lavis/lavis/outputvqa/BLIP2/CE/20230819205/checkpoint_0.pth.
2023-08-20 01:00:16,453 [INFO] Start training
2023-08-20 01:00:16,498 [INFO] Start training epoch 1, 2271 iters per inner epoch.
Train: data epoch: [1]  [   0/2271]  eta: 7:10:06  lr: 0.000005  loss: 1.8329  time: 11.3635  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [  50/2271]  eta: 3:59:00  lr: 0.000005  loss: 2.1252  time: 6.3480  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [ 100/2271]  eta: 3:52:15  lr: 0.000005  loss: 2.0435  time: 6.3793  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [ 150/2271]  eta: 3:46:21  lr: 0.000005  loss: 2.2086  time: 6.3589  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [ 200/2271]  eta: 3:40:37  lr: 0.000005  loss: 2.0135  time: 6.3533  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [ 250/2271]  eta: 3:35:01  lr: 0.000005  loss: 1.4400  time: 6.3560  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [ 300/2271]  eta: 3:29:29  lr: 0.000005  loss: 2.0968  time: 6.3352  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [ 350/2271]  eta: 3:24:03  lr: 0.000005  loss: 2.0411  time: 6.3378  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [ 400/2271]  eta: 3:18:40  lr: 0.000005  loss: 2.3460  time: 6.3498  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [ 450/2271]  eta: 3:13:15  lr: 0.000005  loss: 2.0751  time: 6.3280  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [ 500/2271]  eta: 3:07:54  lr: 0.000005  loss: 1.9998  time: 6.3442  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [ 550/2271]  eta: 3:02:33  lr: 0.000005  loss: 1.7109  time: 6.3436  data: 0.0000  max mem: 15204
Train: data epoch: [1]  [ 600/2271]  eta: 2:57:13  lr: 0.000005  loss: 1.5120  time: 6.3418  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [ 650/2271]  eta: 2:51:51  lr: 0.000005  loss: 2.1140  time: 6.3160  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [ 700/2271]  eta: 2:46:30  lr: 0.000005  loss: 2.0609  time: 6.3485  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [ 750/2271]  eta: 2:41:11  lr: 0.000005  loss: 2.0923  time: 6.3645  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [ 800/2271]  eta: 2:35:53  lr: 0.000005  loss: 2.0234  time: 6.3622  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [ 850/2271]  eta: 2:30:34  lr: 0.000005  loss: 2.2080  time: 6.3374  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [ 900/2271]  eta: 2:25:14  lr: 0.000005  loss: 1.7619  time: 6.3453  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [ 950/2271]  eta: 2:19:54  lr: 0.000005  loss: 2.0618  time: 6.3057  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1000/2271]  eta: 2:14:35  lr: 0.000005  loss: 1.8790  time: 6.3109  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1050/2271]  eta: 2:09:16  lr: 0.000005  loss: 2.1112  time: 6.3399  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1100/2271]  eta: 2:03:58  lr: 0.000005  loss: 1.8902  time: 6.3618  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1150/2271]  eta: 1:58:40  lr: 0.000005  loss: 1.8682  time: 6.3433  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1200/2271]  eta: 1:53:23  lr: 0.000005  loss: 1.9204  time: 6.3512  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1250/2271]  eta: 1:48:05  lr: 0.000005  loss: 1.8250  time: 6.3288  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1300/2271]  eta: 1:42:46  lr: 0.000005  loss: 1.7311  time: 6.3010  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1350/2271]  eta: 1:37:28  lr: 0.000005  loss: 1.9838  time: 6.3486  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1400/2271]  eta: 1:32:11  lr: 0.000005  loss: 2.1835  time: 6.3428  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1450/2271]  eta: 1:26:53  lr: 0.000005  loss: 1.9007  time: 6.3609  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1500/2271]  eta: 1:21:35  lr: 0.000005  loss: 2.1102  time: 6.3714  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1550/2271]  eta: 1:16:17  lr: 0.000005  loss: 1.8798  time: 6.3451  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1600/2271]  eta: 1:11:00  lr: 0.000005  loss: 1.9007  time: 6.3508  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1650/2271]  eta: 1:05:42  lr: 0.000005  loss: 1.8471  time: 6.3314  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1700/2271]  eta: 1:00:24  lr: 0.000005  loss: 2.0419  time: 6.3654  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1750/2271]  eta: 0:55:08  lr: 0.000005  loss: 1.8909  time: 6.4716  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1800/2271]  eta: 0:49:50  lr: 0.000005  loss: 2.1574  time: 6.3567  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1850/2271]  eta: 0:44:33  lr: 0.000005  loss: 1.8686  time: 6.3289  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1900/2271]  eta: 0:39:15  lr: 0.000005  loss: 1.7132  time: 6.3582  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [1950/2271]  eta: 0:33:58  lr: 0.000005  loss: 1.9149  time: 6.3522  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [2000/2271]  eta: 0:28:40  lr: 0.000005  loss: 1.6518  time: 6.3438  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [2050/2271]  eta: 0:23:23  lr: 0.000005  loss: 1.5890  time: 6.3635  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [2100/2271]  eta: 0:18:05  lr: 0.000005  loss: 1.4925  time: 6.3535  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [2150/2271]  eta: 0:12:48  lr: 0.000005  loss: 1.6313  time: 6.3522  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [2200/2271]  eta: 0:07:30  lr: 0.000005  loss: 2.0522  time: 6.3356  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [2250/2271]  eta: 0:02:13  lr: 0.000005  loss: 1.8878  time: 6.3364  data: 0.0000  max mem: 15259
Train: data epoch: [1]  [2270/2271]  eta: 0:00:06  lr: 0.000005  loss: 2.0885  time: 6.3112  data: 0.0000  max mem: 15259
Train: data epoch: [1] Total time: 4:00:16 (6.3481 s / it)
2023-08-20 05:00:33,039 [INFO] Averaged stats: lr: 0.0000  loss: 2.0075
2023-08-20 05:00:33,079 [INFO] No validation splits found.
2023-08-20 05:00:33,200 [INFO] Saving checkpoint at epoch 1 to /public/home/mswanghao/TorchProject/lavis/lavis/outputvqa/BLIP2/CE/20230819205/checkpoint_1.pth.
2023-08-20 05:00:37,541 [INFO] No validation splits found.
2023-08-20 05:00:37,541 [INFO] Training time 8:03:16
