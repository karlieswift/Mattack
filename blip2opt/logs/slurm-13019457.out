WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
loss DRSL3 b=1e-05 start=0 end=20
loss DRSL3 b=1e-05 start=0 end=20
loss DRSL3 b=1e-05 start=0 end=20
loss DRSL3 b=1e-05 start=0 end=20
| distributed init (rank 0, world 4): env://
| distributed init (rank 2, world 4): env://| distributed init (rank 1, world 4): env://

| distributed init (rank 3, world 4): env://
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
2023-08-20 09:15:22,908 [INFO] 
=====  Running Parameters    =====
2023-08-20 09:15:22,909 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 32,
    "batch_size_train": 12,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "inference_method": "generate",
    "init_lr": 1e-05,
    "lr_layer_decay": 0.95,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 2,
    "max_len": 10,
    "min_len": 1,
    "min_lr": 0,
    "num_beams": 5,
    "num_workers": 4,
    "output_dir": "output_vqa/BLIP2/DRSL3_0_20",
    "prompt": "Question: {} Short answer:",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "vqa",
    "train_splits": [
        "train"
    ],
    "warmup_lr": 1e-08,
    "warmup_steps": 1000,
    "weight_decay": 0.05,
    "world_size": 4
}
2023-08-20 09:15:22,909 [INFO] 
======  Dataset Attributes  ======
2023-08-20 09:15:22,909 [INFO] 
======== vg_vqa =======
2023-08-20 09:15:22,909 [INFO] {
    "build_info": {
        "annotations": {
            "train": {
                "storage": "vg/annotations/vg_qa.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/visual_genome/vg_qa.json"
            }
        },
        "images": {
            "storage": "vg/images/"
        }
    },
    "data_type": "images",
    "text_processor": {
        "train": {
            "name": "blip_question"
        }
    },
    "vis_processor": {
        "train": {
            "image_size": 400,
            "name": "blip_image_train"
        }
    }
}
2023-08-20 09:15:22,909 [INFO] 
======  Model Attributes  ======
2023-08-20 09:15:22,910 [INFO] {
    "arch": "blip2_opt",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 400,
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "pretrain_opt2.7b",
    "num_query_token": 32,
    "opt_model": "facebook/opt-2.7b",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth",
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_model": "eva_clip_g",
    "vit_precision": "fp32"
}
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/vg/annotations/vg_qa.json
2023-08-20 09:15:22,925 [INFO] Building datasets...
BlipQuestionProcessor
Position interpolate from 16x16 to 28x28
2023-08-20 09:16:00,877 [INFO] freeze vision encoder
https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-20 09:19:23,227 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_opt2.7b.pth
2023-08-20 09:19:23,257 [INFO] Start training
2023-08-20 09:19:44,316 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-08-20 09:19:44,319 [INFO] Loaded 145395 records for train split from the dataset.
2023-08-20 09:19:44,391 [INFO] number of trainable parameters: 107133696
2023-08-20 09:19:44,392 [INFO] Start training epoch 0, 3029 iters per inner epoch.
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
Train: data epoch: [0]  [   0/3029]  eta: 22:01:05  lr: 0.000000  loss: 2.7897  time: 26.1689  data: 0.0000  max mem: 12766
2023-08-20 09:20:10,624 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  50/3029]  eta: 4:39:16  lr: 0.000001  loss: 2.6828  time: 5.2282  data: 0.0000  max mem: 14218
Train: data epoch: [0]  [ 100/3029]  eta: 4:24:25  lr: 0.000001  loss: 2.5749  time: 5.2073  data: 0.0000  max mem: 14218
Train: data epoch: [0]  [ 150/3029]  eta: 4:16:43  lr: 0.000002  loss: 2.5273  time: 5.2112  data: 0.0000  max mem: 14288
Train: data epoch: [0]  [ 200/3029]  eta: 4:11:01  lr: 0.000002  loss: 1.7839  time: 5.2408  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 250/3029]  eta: 4:05:37  lr: 0.000003  loss: 1.8293  time: 5.2262  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 300/3029]  eta: 4:00:34  lr: 0.000003  loss: 2.2252  time: 5.2099  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 350/3029]  eta: 3:56:02  lr: 0.000004  loss: 2.0734  time: 5.3101  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 400/3029]  eta: 3:51:29  lr: 0.000004  loss: 2.3758  time: 5.2308  data: 0.0000  max mem: 14385
Train: data epoch: [0]  [ 450/3029]  eta: 3:47:05  lr: 0.000005  loss: 2.0196  time: 5.3015  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 500/3029]  eta: 3:42:42  lr: 0.000005  loss: 2.0682  time: 5.2887  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 550/3029]  eta: 3:38:21  lr: 0.000006  loss: 1.5421  time: 5.3006  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 600/3029]  eta: 3:33:57  lr: 0.000006  loss: 2.1563  time: 5.2955  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 650/3029]  eta: 3:29:34  lr: 0.000007  loss: 2.4635  time: 5.3063  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 700/3029]  eta: 3:25:11  lr: 0.000007  loss: 1.8848  time: 5.3172  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 750/3029]  eta: 3:20:43  lr: 0.000008  loss: 2.2306  time: 5.2907  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 800/3029]  eta: 3:16:23  lr: 0.000008  loss: 1.8753  time: 5.3015  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 850/3029]  eta: 3:11:59  lr: 0.000009  loss: 2.3051  time: 5.2873  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 900/3029]  eta: 3:07:35  lr: 0.000009  loss: 2.2492  time: 5.2789  data: 0.0000  max mem: 14431
Train: data epoch: [0]  [ 950/3029]  eta: 3:03:12  lr: 0.000010  loss: 2.2937  time: 5.3253  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1000/3029]  eta: 2:58:47  lr: 0.000010  loss: 1.7544  time: 5.2631  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1050/3029]  eta: 2:54:21  lr: 0.000010  loss: 2.0265  time: 5.2508  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1100/3029]  eta: 2:49:56  lr: 0.000010  loss: 2.1788  time: 5.2929  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1150/3029]  eta: 2:45:31  lr: 0.000010  loss: 2.0936  time: 5.2883  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1200/3029]  eta: 2:41:07  lr: 0.000010  loss: 2.0508  time: 5.2800  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1250/3029]  eta: 2:36:42  lr: 0.000010  loss: 2.2813  time: 5.2845  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1300/3029]  eta: 2:32:18  lr: 0.000010  loss: 2.0375  time: 5.2985  data: 0.0000  max mem: 14434
Train: data epoch: [0]  [1350/3029]  eta: 2:27:53  lr: 0.000010  loss: 1.8791  time: 5.2929  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1400/3029]  eta: 2:23:30  lr: 0.000010  loss: 1.9097  time: 5.2743  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1450/3029]  eta: 2:19:04  lr: 0.000010  loss: 1.9995  time: 5.2659  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1500/3029]  eta: 2:14:41  lr: 0.000010  loss: 2.1030  time: 5.2996  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1550/3029]  eta: 2:10:16  lr: 0.000010  loss: 1.9241  time: 5.2612  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1600/3029]  eta: 2:05:51  lr: 0.000010  loss: 2.1367  time: 5.2680  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1650/3029]  eta: 2:01:26  lr: 0.000010  loss: 1.9643  time: 5.2554  data: 0.0000  max mem: 14483
Train: data epoch: [0]  [1700/3029]  eta: 1:57:01  lr: 0.000010  loss: 2.4442  time: 5.2083  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1750/3029]  eta: 1:52:35  lr: 0.000010  loss: 1.9923  time: 5.2376  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1800/3029]  eta: 1:48:09  lr: 0.000010  loss: 1.7809  time: 5.2121  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1850/3029]  eta: 1:43:43  lr: 0.000010  loss: 1.9284  time: 5.2324  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1900/3029]  eta: 1:39:17  lr: 0.000010  loss: 2.2990  time: 5.2192  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [1950/3029]  eta: 1:34:51  lr: 0.000010  loss: 1.8515  time: 5.2422  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2000/3029]  eta: 1:30:27  lr: 0.000010  loss: 1.6999  time: 5.2150  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2050/3029]  eta: 1:26:01  lr: 0.000010  loss: 2.1594  time: 5.1923  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2100/3029]  eta: 1:21:37  lr: 0.000010  loss: 2.1305  time: 5.2336  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2150/3029]  eta: 1:17:12  lr: 0.000010  loss: 2.3365  time: 5.2221  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2200/3029]  eta: 1:12:47  lr: 0.000010  loss: 2.2534  time: 5.1971  data: 0.0000  max mem: 14487
Train: data epoch: [0]  [2250/3029]  eta: 1:08:23  lr: 0.000010  loss: 1.9661  time: 5.2472  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2300/3029]  eta: 1:03:59  lr: 0.000010  loss: 2.4322  time: 5.2241  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2350/3029]  eta: 0:59:35  lr: 0.000010  loss: 2.2958  time: 5.1988  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2400/3029]  eta: 0:55:11  lr: 0.000010  loss: 2.1341  time: 5.1975  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2450/3029]  eta: 0:50:48  lr: 0.000010  loss: 1.8167  time: 5.2164  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2500/3029]  eta: 0:46:24  lr: 0.000010  loss: 2.0838  time: 5.2301  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2550/3029]  eta: 0:42:00  lr: 0.000010  loss: 2.1052  time: 5.2502  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2600/3029]  eta: 0:37:37  lr: 0.000010  loss: 2.1596  time: 5.2235  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2650/3029]  eta: 0:33:14  lr: 0.000010  loss: 1.8490  time: 5.2285  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2700/3029]  eta: 0:28:50  lr: 0.000010  loss: 2.1926  time: 5.2333  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2750/3029]  eta: 0:24:27  lr: 0.000010  loss: 1.8300  time: 5.2243  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2800/3029]  eta: 0:20:04  lr: 0.000010  loss: 1.8342  time: 5.2230  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2850/3029]  eta: 0:15:41  lr: 0.000010  loss: 1.7086  time: 5.2259  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2900/3029]  eta: 0:11:18  lr: 0.000010  loss: 1.7813  time: 5.2271  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [2950/3029]  eta: 0:06:55  lr: 0.000010  loss: 2.2617  time: 5.2172  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [3000/3029]  eta: 0:02:32  lr: 0.000010  loss: 2.0338  time: 5.2164  data: 0.0000  max mem: 14563
Train: data epoch: [0]  [3028/3029]  eta: 0:00:05  lr: 0.000010  loss: 1.9473  time: 5.2085  data: 0.0000  max mem: 14563
Train: data epoch: [0] Total time: 4:25:23 (5.2571 s / it)
2023-08-20 13:45:08,271 [INFO] Averaged stats: lr: 0.0000  loss: 2.0567
2023-08-20 13:45:08,340 [INFO] No validation splits found.
2023-08-20 13:45:08,392 [INFO] Saving checkpoint at epoch 0 to /public/home/mswanghao/TorchProject/lavis/lavis/output_vqa/BLIP2/DRSL3_0_20/20230820091/checkpoint_0.pth.
2023-08-20 13:45:14,454 [INFO] Start training
2023-08-20 13:45:14,499 [INFO] Start training epoch 1, 3029 iters per inner epoch.
Train: data epoch: [1]  [   0/3029]  eta: 9:33:58  lr: 0.000005  loss: 1.8601  time: 11.3697  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [  50/3029]  eta: 4:24:19  lr: 0.000005  loss: 1.7703  time: 5.1955  data: 0.0001  max mem: 14563
Train: data epoch: [1]  [ 100/3029]  eta: 4:17:38  lr: 0.000005  loss: 1.7567  time: 5.2350  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 150/3029]  eta: 4:12:34  lr: 0.000005  loss: 2.5070  time: 5.2257  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 200/3029]  eta: 4:07:34  lr: 0.000005  loss: 2.1253  time: 5.2072  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 250/3029]  eta: 4:02:49  lr: 0.000005  loss: 1.7484  time: 5.2149  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 300/3029]  eta: 3:58:20  lr: 0.000005  loss: 2.0264  time: 5.2178  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 350/3029]  eta: 3:53:50  lr: 0.000005  loss: 1.8685  time: 5.2340  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 400/3029]  eta: 3:49:25  lr: 0.000005  loss: 2.0065  time: 5.2101  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 450/3029]  eta: 3:44:52  lr: 0.000005  loss: 1.7379  time: 5.1940  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 500/3029]  eta: 3:40:30  lr: 0.000005  loss: 2.1538  time: 5.2532  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 550/3029]  eta: 3:36:07  lr: 0.000005  loss: 1.9906  time: 5.2176  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 600/3029]  eta: 3:31:41  lr: 0.000005  loss: 2.2784  time: 5.1918  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 650/3029]  eta: 3:27:19  lr: 0.000005  loss: 1.9333  time: 5.2132  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 700/3029]  eta: 3:22:55  lr: 0.000005  loss: 1.8509  time: 5.2020  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 750/3029]  eta: 3:18:32  lr: 0.000005  loss: 2.2566  time: 5.2202  data: 0.0000  max mem: 14563
Train: data epoch: [1]  [ 800/3029]  eta: 3:14:11  lr: 0.000005  loss: 1.4976  time: 5.2219  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 850/3029]  eta: 3:09:47  lr: 0.000005  loss: 1.6695  time: 5.2368  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 900/3029]  eta: 3:05:26  lr: 0.000005  loss: 1.9436  time: 5.2260  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [ 950/3029]  eta: 3:01:07  lr: 0.000005  loss: 1.7542  time: 5.2537  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1000/3029]  eta: 2:56:45  lr: 0.000005  loss: 2.0153  time: 5.2261  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1050/3029]  eta: 2:52:23  lr: 0.000005  loss: 2.0533  time: 5.2142  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1100/3029]  eta: 2:48:02  lr: 0.000005  loss: 1.9019  time: 5.2401  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1150/3029]  eta: 2:43:40  lr: 0.000005  loss: 2.0226  time: 5.2103  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1200/3029]  eta: 2:39:18  lr: 0.000005  loss: 1.7878  time: 5.2087  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1250/3029]  eta: 2:34:55  lr: 0.000005  loss: 1.8901  time: 5.2113  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1300/3029]  eta: 2:30:34  lr: 0.000005  loss: 1.9696  time: 5.2394  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1350/3029]  eta: 2:26:13  lr: 0.000005  loss: 1.6761  time: 5.2302  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1400/3029]  eta: 2:21:57  lr: 0.000005  loss: 1.9580  time: 5.4493  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1450/3029]  eta: 2:17:44  lr: 0.000005  loss: 1.8896  time: 5.2789  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1500/3029]  eta: 2:13:22  lr: 0.000005  loss: 2.2179  time: 5.2395  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1550/3029]  eta: 2:09:00  lr: 0.000005  loss: 1.7213  time: 5.1978  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1600/3029]  eta: 2:04:41  lr: 0.000005  loss: 1.7963  time: 5.4376  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1650/3029]  eta: 2:00:20  lr: 0.000005  loss: 2.0718  time: 5.2136  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1700/3029]  eta: 1:55:58  lr: 0.000005  loss: 1.8875  time: 5.2090  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1750/3029]  eta: 1:51:36  lr: 0.000005  loss: 1.9403  time: 5.2147  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1800/3029]  eta: 1:47:13  lr: 0.000005  loss: 2.1009  time: 5.1774  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1850/3029]  eta: 1:42:51  lr: 0.000005  loss: 2.1354  time: 5.2062  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1900/3029]  eta: 1:38:30  lr: 0.000005  loss: 2.2619  time: 5.2948  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [1950/3029]  eta: 1:34:11  lr: 0.000005  loss: 1.8605  time: 5.3306  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2000/3029]  eta: 1:29:51  lr: 0.000005  loss: 2.1084  time: 5.3364  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2050/3029]  eta: 1:25:32  lr: 0.000005  loss: 1.7930  time: 5.3142  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2100/3029]  eta: 1:21:12  lr: 0.000005  loss: 1.9567  time: 5.3310  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2150/3029]  eta: 1:16:51  lr: 0.000005  loss: 2.0853  time: 5.3415  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2200/3029]  eta: 1:12:31  lr: 0.000005  loss: 1.8091  time: 5.3407  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2250/3029]  eta: 1:08:10  lr: 0.000005  loss: 1.9613  time: 5.3536  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2300/3029]  eta: 1:03:49  lr: 0.000005  loss: 2.1736  time: 5.3461  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2350/3029]  eta: 0:59:28  lr: 0.000005  loss: 1.9721  time: 5.3177  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2400/3029]  eta: 0:55:06  lr: 0.000005  loss: 2.1763  time: 5.3283  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2450/3029]  eta: 0:50:44  lr: 0.000005  loss: 2.0576  time: 5.3368  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2500/3029]  eta: 0:46:22  lr: 0.000005  loss: 2.0040  time: 5.3282  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2550/3029]  eta: 0:42:00  lr: 0.000005  loss: 2.3300  time: 5.3280  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2600/3029]  eta: 0:37:37  lr: 0.000005  loss: 1.9790  time: 5.3292  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2650/3029]  eta: 0:33:14  lr: 0.000005  loss: 1.8852  time: 5.3545  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2700/3029]  eta: 0:28:52  lr: 0.000005  loss: 2.1260  time: 5.3287  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2750/3029]  eta: 0:24:29  lr: 0.000005  loss: 2.4186  time: 5.3250  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2800/3029]  eta: 0:20:06  lr: 0.000005  loss: 1.5653  time: 5.3512  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2850/3029]  eta: 0:15:43  lr: 0.000005  loss: 1.5901  time: 5.3023  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2900/3029]  eta: 0:11:19  lr: 0.000005  loss: 1.9877  time: 5.3242  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [2950/3029]  eta: 0:06:56  lr: 0.000005  loss: 2.1949  time: 5.3037  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [3000/3029]  eta: 0:02:32  lr: 0.000005  loss: 2.0322  time: 5.3315  data: 0.0000  max mem: 14580
Train: data epoch: [1]  [3028/3029]  eta: 0:00:05  lr: 0.000005  loss: 2.1484  time: 5.3252  data: 0.0000  max mem: 14580
Train: data epoch: [1] Total time: 4:26:08 (5.2717 s / it)
2023-08-20 18:11:22,565 [INFO] Averaged stats: lr: 0.0000  loss: 2.0142
2023-08-20 18:11:22,612 [INFO] No validation splits found.
2023-08-20 18:11:22,669 [INFO] Saving checkpoint at epoch 1 to /public/home/mswanghao/TorchProject/lavis/lavis/output_vqa/BLIP2/DRSL3_0_20/20230820091/checkpoint_1.pth.
2023-08-20 18:11:28,425 [INFO] No validation splits found.
2023-08-20 18:11:28,426 [INFO] Training time 8:52:05
