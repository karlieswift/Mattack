WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
loss CE b=1e-05 start=0 end=10loss CE b=1e-05 start=0 end=10

loss CE b=1e-05 start=0 end=10
loss CE b=1e-05 start=0 end=10
| distributed init (rank 3, world 4): env://
| distributed init (rank 2, world 4): env://
| distributed init (rank 1, world 4): env://
| distributed init (rank 0, world 4): env://
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
2023-08-15 11:36:42,545 [INFO] 
=====  Running Parameters    =====
2023-08-15 11:36:42,546 [INFO] {
    "amp": true,
    "batch_size_eval": 2,
    "batch_size_train": 16,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 0.0001,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 1,
    "min_lr": 1e-05,
    "num_workers": 4,
    "output_dir": "output/BLIP2/CEPretrain_stage2",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "image_text_pretrain",
    "train_splits": [
        "train"
    ],
    "warmup_lr": 1e-06,
    "warmup_steps": 2000,
    "weight_decay": 0.05,
    "world_size": 4
}
2023-08-15 11:36:42,546 [INFO] 
======  Dataset Attributes  ======
2023-08-15 11:36:42,546 [INFO] 
======== coco_caption =======
2023-08-15 11:36:42,547 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "coco/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "coco/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "coco/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "coco/images/"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "train": {
            "name": "blip_caption"
        }
    },
    "vis_processor": {
        "train": {
            "image_size": 224,
            "name": "blip2_image_train"
        }
    }
}
2023-08-15 11:36:42,547 [INFO] 
======  Model Attributes  ======
2023-08-15 11:36:42,547 [INFO] {
    "arch": "blip2_opt",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 224,
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "pretrain_opt2.7b",
    "num_query_token": 32,
    "opt_model": "facebook/opt-2.7b",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained.pth",
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_precision": "fp16"
}
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/coco/annotations/coco_karpathy_train.json
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/coco/annotations/coco_karpathy_val.json
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/coco/annotations/coco_karpathy_test.json
2023-08-15 11:36:42,560 [INFO] Building datasets...
2023-08-15 11:37:24,430 [INFO] freeze vision encoder
2023-08-15 11:40:48,783 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained.pth
2023-08-15 11:40:48,830 [INFO] Start training
2023-08-15 11:41:06,305 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-08-15 11:41:06,313 [INFO] Loaded 566747 records for train split from the dataset.
2023-08-15 11:41:06,313 [INFO] Loaded 5000 records for val split from the dataset.
2023-08-15 11:41:06,313 [INFO] Loaded 5000 records for test split from the dataset.
2023-08-15 11:41:06,365 [INFO] number of trainable parameters: 107133696
2023-08-15 11:41:06,367 [INFO] Start training epoch 0, 8855 iters per inner epoch.
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
Train: data epoch: [0]  [   0/8855]  eta: 2 days, 12:27:15  lr: 0.000001  loss: 6.2707  time: 24.5777  data: 0.0000  max mem: 11004
2023-08-15 11:41:31,001 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  50/8855]  eta: 9:35:52  lr: 0.000003  loss: 4.4624  time: 3.4806  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 100/8855]  eta: 8:58:06  lr: 0.000006  loss: 4.0315  time: 3.4826  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 150/8855]  eta: 8:43:45  lr: 0.000008  loss: 4.0377  time: 3.4514  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 200/8855]  eta: 8:34:06  lr: 0.000011  loss: 3.6188  time: 3.4406  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 250/8855]  eta: 8:27:32  lr: 0.000013  loss: 3.8790  time: 3.4596  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 300/8855]  eta: 8:22:21  lr: 0.000016  loss: 3.1691  time: 3.4657  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 350/8855]  eta: 8:17:31  lr: 0.000018  loss: 2.7607  time: 3.4227  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 400/8855]  eta: 8:13:13  lr: 0.000021  loss: 2.6971  time: 3.4389  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 450/8855]  eta: 8:09:46  lr: 0.000023  loss: 2.5860  time: 3.4877  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 500/8855]  eta: 8:05:52  lr: 0.000026  loss: 2.7512  time: 3.4019  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 550/8855]  eta: 8:01:50  lr: 0.000028  loss: 2.6294  time: 3.3908  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 600/8855]  eta: 7:58:43  lr: 0.000031  loss: 2.6358  time: 3.4635  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 650/8855]  eta: 7:55:37  lr: 0.000033  loss: 2.4978  time: 3.4592  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 700/8855]  eta: 7:52:19  lr: 0.000036  loss: 2.6953  time: 3.4526  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 750/8855]  eta: 7:48:57  lr: 0.000038  loss: 2.4722  time: 3.4156  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 800/8855]  eta: 7:45:50  lr: 0.000041  loss: 2.2816  time: 3.4455  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 850/8855]  eta: 7:42:49  lr: 0.000043  loss: 2.6812  time: 3.4382  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 900/8855]  eta: 7:39:58  lr: 0.000046  loss: 2.6256  time: 3.4937  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [ 950/8855]  eta: 7:36:55  lr: 0.000048  loss: 2.3625  time: 3.4785  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1000/8855]  eta: 7:33:52  lr: 0.000051  loss: 2.3063  time: 3.4262  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1050/8855]  eta: 7:30:46  lr: 0.000053  loss: 2.4701  time: 3.4423  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1100/8855]  eta: 7:27:56  lr: 0.000055  loss: 2.7990  time: 3.4791  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1150/8855]  eta: 7:24:56  lr: 0.000058  loss: 2.2065  time: 3.4208  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1200/8855]  eta: 7:21:57  lr: 0.000060  loss: 2.8864  time: 3.4714  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1250/8855]  eta: 7:18:59  lr: 0.000063  loss: 2.3692  time: 3.4492  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1300/8855]  eta: 7:15:59  lr: 0.000065  loss: 2.8150  time: 3.4314  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1350/8855]  eta: 7:13:01  lr: 0.000068  loss: 2.6861  time: 3.4392  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1400/8855]  eta: 7:10:05  lr: 0.000070  loss: 2.2713  time: 3.4704  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1450/8855]  eta: 7:07:04  lr: 0.000073  loss: 2.3425  time: 3.4201  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1500/8855]  eta: 7:04:08  lr: 0.000075  loss: 2.3876  time: 3.4395  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1550/8855]  eta: 7:01:07  lr: 0.000078  loss: 2.2976  time: 3.4408  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1600/8855]  eta: 6:58:04  lr: 0.000080  loss: 2.7597  time: 3.3891  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1650/8855]  eta: 6:55:11  lr: 0.000083  loss: 2.2792  time: 3.4982  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1700/8855]  eta: 6:52:16  lr: 0.000085  loss: 2.1256  time: 3.4575  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1750/8855]  eta: 6:49:14  lr: 0.000088  loss: 2.2688  time: 3.3630  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1800/8855]  eta: 6:46:19  lr: 0.000090  loss: 2.2271  time: 3.4320  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1850/8855]  eta: 6:43:25  lr: 0.000093  loss: 2.4067  time: 3.4603  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1900/8855]  eta: 6:40:28  lr: 0.000095  loss: 2.4729  time: 3.4340  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [1950/8855]  eta: 6:37:37  lr: 0.000098  loss: 2.2776  time: 3.4388  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2000/8855]  eta: 6:34:33  lr: 0.000100  loss: 2.4025  time: 3.4234  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2050/8855]  eta: 6:31:37  lr: 0.000100  loss: 2.5735  time: 3.4083  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2100/8855]  eta: 6:28:40  lr: 0.000100  loss: 2.3955  time: 3.4400  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2150/8855]  eta: 6:25:42  lr: 0.000100  loss: 2.2988  time: 3.4229  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2200/8855]  eta: 6:22:57  lr: 0.000100  loss: 2.1278  time: 3.4268  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2250/8855]  eta: 6:19:57  lr: 0.000100  loss: 2.0090  time: 3.4188  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2300/8855]  eta: 6:17:03  lr: 0.000100  loss: 2.4516  time: 3.4368  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2350/8855]  eta: 6:14:11  lr: 0.000100  loss: 2.1667  time: 3.4619  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2400/8855]  eta: 6:11:21  lr: 0.000100  loss: 2.4856  time: 3.4995  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2450/8855]  eta: 6:08:32  lr: 0.000100  loss: 2.1862  time: 3.4415  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2500/8855]  eta: 6:05:36  lr: 0.000100  loss: 2.1919  time: 3.4412  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2550/8855]  eta: 6:02:44  lr: 0.000100  loss: 2.4701  time: 3.4729  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2600/8855]  eta: 5:59:50  lr: 0.000100  loss: 1.8340  time: 3.4002  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2650/8855]  eta: 5:56:56  lr: 0.000100  loss: 2.4831  time: 3.4355  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2700/8855]  eta: 5:54:03  lr: 0.000100  loss: 2.3253  time: 3.4518  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2750/8855]  eta: 5:51:09  lr: 0.000100  loss: 2.0731  time: 3.3725  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2800/8855]  eta: 5:48:16  lr: 0.000100  loss: 2.3015  time: 3.4147  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2850/8855]  eta: 5:45:24  lr: 0.000100  loss: 2.0870  time: 3.4673  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2900/8855]  eta: 5:42:29  lr: 0.000100  loss: 2.2454  time: 3.4257  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [2950/8855]  eta: 5:39:33  lr: 0.000100  loss: 1.9043  time: 3.4119  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3000/8855]  eta: 5:36:38  lr: 0.000100  loss: 2.3789  time: 3.3955  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3050/8855]  eta: 5:33:43  lr: 0.000100  loss: 2.5694  time: 3.4275  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3100/8855]  eta: 5:30:48  lr: 0.000100  loss: 2.3322  time: 3.4623  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3150/8855]  eta: 5:27:55  lr: 0.000100  loss: 2.1421  time: 3.4479  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3200/8855]  eta: 5:24:58  lr: 0.000100  loss: 2.2016  time: 3.3896  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3250/8855]  eta: 5:22:03  lr: 0.000100  loss: 1.9709  time: 3.4205  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3300/8855]  eta: 5:19:09  lr: 0.000100  loss: 2.3356  time: 3.4236  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3350/8855]  eta: 5:16:16  lr: 0.000100  loss: 2.0598  time: 3.3733  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3400/8855]  eta: 5:13:23  lr: 0.000100  loss: 1.9664  time: 3.4434  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3450/8855]  eta: 5:10:33  lr: 0.000100  loss: 2.3586  time: 3.4606  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3500/8855]  eta: 5:07:40  lr: 0.000100  loss: 2.2320  time: 3.4360  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3550/8855]  eta: 5:04:48  lr: 0.000100  loss: 2.1767  time: 3.4337  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3600/8855]  eta: 5:01:54  lr: 0.000100  loss: 2.2869  time: 3.4265  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3650/8855]  eta: 4:59:00  lr: 0.000100  loss: 2.3763  time: 3.4445  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3700/8855]  eta: 4:56:08  lr: 0.000100  loss: 2.3426  time: 3.4238  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3750/8855]  eta: 4:53:16  lr: 0.000100  loss: 2.2630  time: 3.4410  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3800/8855]  eta: 4:50:24  lr: 0.000100  loss: 2.3654  time: 3.4414  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3850/8855]  eta: 4:47:30  lr: 0.000100  loss: 2.3504  time: 3.4196  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3900/8855]  eta: 4:44:37  lr: 0.000100  loss: 2.0092  time: 3.4331  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [3950/8855]  eta: 4:41:45  lr: 0.000100  loss: 2.0861  time: 3.4585  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4000/8855]  eta: 4:38:53  lr: 0.000100  loss: 2.3587  time: 3.4418  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4050/8855]  eta: 4:35:59  lr: 0.000100  loss: 2.4542  time: 3.4184  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4100/8855]  eta: 4:33:09  lr: 0.000100  loss: 2.4943  time: 3.4528  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4150/8855]  eta: 4:30:17  lr: 0.000100  loss: 2.1260  time: 3.4693  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4200/8855]  eta: 4:27:24  lr: 0.000100  loss: 2.0386  time: 3.4212  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4250/8855]  eta: 4:24:31  lr: 0.000100  loss: 2.1535  time: 3.4685  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4300/8855]  eta: 4:21:40  lr: 0.000100  loss: 2.0594  time: 3.4966  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4350/8855]  eta: 4:18:47  lr: 0.000100  loss: 2.1647  time: 3.4530  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4400/8855]  eta: 4:15:54  lr: 0.000100  loss: 2.3013  time: 3.4004  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4450/8855]  eta: 4:13:03  lr: 0.000100  loss: 2.0352  time: 3.4946  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4500/8855]  eta: 4:10:10  lr: 0.000100  loss: 2.6156  time: 3.4013  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4550/8855]  eta: 4:07:17  lr: 0.000100  loss: 2.3222  time: 3.4409  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4600/8855]  eta: 4:04:26  lr: 0.000100  loss: 2.2384  time: 3.4634  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4650/8855]  eta: 4:01:32  lr: 0.000100  loss: 2.1230  time: 3.4344  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4700/8855]  eta: 3:58:42  lr: 0.000100  loss: 2.0392  time: 3.4456  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4750/8855]  eta: 3:55:51  lr: 0.000100  loss: 2.5613  time: 3.4507  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4800/8855]  eta: 3:52:57  lr: 0.000100  loss: 2.2389  time: 3.3977  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4850/8855]  eta: 3:50:05  lr: 0.000100  loss: 1.9716  time: 3.4542  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4900/8855]  eta: 3:47:13  lr: 0.000100  loss: 2.4121  time: 3.4550  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [4950/8855]  eta: 3:44:19  lr: 0.000100  loss: 2.0646  time: 3.4180  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5000/8855]  eta: 3:41:26  lr: 0.000100  loss: 1.9723  time: 3.4054  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5050/8855]  eta: 3:38:33  lr: 0.000100  loss: 2.4460  time: 3.4290  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5100/8855]  eta: 3:35:42  lr: 0.000100  loss: 2.0690  time: 3.4779  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5150/8855]  eta: 3:32:52  lr: 0.000100  loss: 2.3677  time: 3.5033  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5200/8855]  eta: 3:29:59  lr: 0.000100  loss: 2.1925  time: 3.4159  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5250/8855]  eta: 3:27:08  lr: 0.000100  loss: 1.9974  time: 3.4317  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5300/8855]  eta: 3:24:17  lr: 0.000100  loss: 2.0809  time: 3.4666  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5350/8855]  eta: 3:21:24  lr: 0.000100  loss: 2.0504  time: 3.4759  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5400/8855]  eta: 3:18:31  lr: 0.000100  loss: 1.9880  time: 3.4060  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5450/8855]  eta: 3:15:39  lr: 0.000100  loss: 2.1404  time: 3.4077  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5500/8855]  eta: 3:12:46  lr: 0.000100  loss: 2.1336  time: 3.4280  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5550/8855]  eta: 3:09:54  lr: 0.000100  loss: 1.8594  time: 3.4679  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5600/8855]  eta: 3:07:01  lr: 0.000100  loss: 1.9138  time: 3.4296  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5650/8855]  eta: 3:04:09  lr: 0.000100  loss: 2.1169  time: 3.4366  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5700/8855]  eta: 3:01:16  lr: 0.000100  loss: 2.3011  time: 3.4117  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5750/8855]  eta: 2:58:24  lr: 0.000100  loss: 2.1730  time: 3.4980  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5800/8855]  eta: 2:55:31  lr: 0.000100  loss: 1.8317  time: 3.4405  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5850/8855]  eta: 2:52:39  lr: 0.000100  loss: 2.2247  time: 3.4152  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5900/8855]  eta: 2:49:47  lr: 0.000100  loss: 1.7670  time: 3.4892  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [5950/8855]  eta: 2:46:54  lr: 0.000100  loss: 2.6364  time: 3.4647  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6000/8855]  eta: 2:44:02  lr: 0.000100  loss: 2.0321  time: 3.4911  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6050/8855]  eta: 2:41:10  lr: 0.000100  loss: 2.3404  time: 3.4735  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6100/8855]  eta: 2:38:18  lr: 0.000100  loss: 2.1595  time: 3.4681  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6150/8855]  eta: 2:35:26  lr: 0.000100  loss: 2.2923  time: 3.4793  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6200/8855]  eta: 2:32:32  lr: 0.000100  loss: 2.3086  time: 3.3764  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6250/8855]  eta: 2:29:40  lr: 0.000100  loss: 2.1310  time: 3.4125  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6300/8855]  eta: 2:26:48  lr: 0.000100  loss: 2.0389  time: 3.4848  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6350/8855]  eta: 2:23:55  lr: 0.000100  loss: 2.1656  time: 3.4261  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6400/8855]  eta: 2:21:03  lr: 0.000100  loss: 2.2999  time: 3.4367  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6450/8855]  eta: 2:18:11  lr: 0.000100  loss: 2.2897  time: 3.4253  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6500/8855]  eta: 2:15:18  lr: 0.000100  loss: 2.2285  time: 3.4428  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6550/8855]  eta: 2:12:26  lr: 0.000100  loss: 2.5907  time: 3.5033  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6600/8855]  eta: 2:09:33  lr: 0.000100  loss: 2.0963  time: 3.4489  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6650/8855]  eta: 2:06:40  lr: 0.000100  loss: 2.1149  time: 3.4054  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6700/8855]  eta: 2:03:48  lr: 0.000100  loss: 2.0540  time: 3.4832  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6750/8855]  eta: 2:00:56  lr: 0.000100  loss: 2.4531  time: 3.4847  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6800/8855]  eta: 1:58:04  lr: 0.000100  loss: 2.0571  time: 3.4947  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6850/8855]  eta: 1:55:12  lr: 0.000100  loss: 1.8363  time: 3.4966  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6900/8855]  eta: 1:52:19  lr: 0.000100  loss: 1.9327  time: 3.4261  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [6950/8855]  eta: 1:49:27  lr: 0.000100  loss: 2.2132  time: 3.4536  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7000/8855]  eta: 1:46:34  lr: 0.000100  loss: 2.0027  time: 3.3507  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7050/8855]  eta: 1:43:42  lr: 0.000100  loss: 2.1300  time: 3.4239  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7100/8855]  eta: 1:40:50  lr: 0.000100  loss: 2.0001  time: 3.4544  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7150/8855]  eta: 1:37:57  lr: 0.000100  loss: 2.4172  time: 3.4303  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7200/8855]  eta: 1:35:04  lr: 0.000100  loss: 1.9736  time: 3.4751  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7250/8855]  eta: 1:32:12  lr: 0.000100  loss: 2.0458  time: 3.5141  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7300/8855]  eta: 1:29:20  lr: 0.000100  loss: 2.2639  time: 3.4270  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7350/8855]  eta: 1:26:27  lr: 0.000100  loss: 2.3312  time: 3.4482  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7400/8855]  eta: 1:23:35  lr: 0.000100  loss: 2.4041  time: 3.4260  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7450/8855]  eta: 1:20:43  lr: 0.000100  loss: 2.3230  time: 3.4393  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7500/8855]  eta: 1:17:50  lr: 0.000100  loss: 2.3720  time: 3.4472  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7550/8855]  eta: 1:14:58  lr: 0.000100  loss: 2.3619  time: 3.4773  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7600/8855]  eta: 1:12:06  lr: 0.000100  loss: 2.4375  time: 3.4662  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7650/8855]  eta: 1:09:13  lr: 0.000100  loss: 2.1907  time: 3.4398  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7700/8855]  eta: 1:06:21  lr: 0.000100  loss: 2.2626  time: 3.4018  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7750/8855]  eta: 1:03:29  lr: 0.000100  loss: 1.8512  time: 3.4786  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7800/8855]  eta: 1:00:36  lr: 0.000100  loss: 2.2054  time: 3.3790  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7850/8855]  eta: 0:57:44  lr: 0.000100  loss: 2.1438  time: 3.4652  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7900/8855]  eta: 0:54:52  lr: 0.000100  loss: 1.9713  time: 3.4486  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [7950/8855]  eta: 0:51:59  lr: 0.000100  loss: 2.3957  time: 3.4549  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8000/8855]  eta: 0:49:07  lr: 0.000100  loss: 2.1414  time: 3.4407  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8050/8855]  eta: 0:46:14  lr: 0.000100  loss: 2.4130  time: 3.4369  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8100/8855]  eta: 0:43:22  lr: 0.000100  loss: 2.2783  time: 3.4076  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8150/8855]  eta: 0:40:30  lr: 0.000100  loss: 1.9234  time: 3.4412  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8200/8855]  eta: 0:37:37  lr: 0.000100  loss: 2.2129  time: 3.4742  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8250/8855]  eta: 0:34:45  lr: 0.000100  loss: 1.9991  time: 3.4207  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8300/8855]  eta: 0:31:53  lr: 0.000100  loss: 2.1076  time: 3.4746  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8350/8855]  eta: 0:29:00  lr: 0.000100  loss: 2.3857  time: 3.4538  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8400/8855]  eta: 0:26:08  lr: 0.000100  loss: 2.0211  time: 3.5057  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8450/8855]  eta: 0:23:16  lr: 0.000100  loss: 1.9975  time: 3.4335  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8500/8855]  eta: 0:20:23  lr: 0.000100  loss: 2.2650  time: 3.4987  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8550/8855]  eta: 0:17:31  lr: 0.000100  loss: 2.2539  time: 3.4197  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8600/8855]  eta: 0:14:38  lr: 0.000100  loss: 1.8864  time: 3.4626  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8650/8855]  eta: 0:11:46  lr: 0.000100  loss: 2.1816  time: 3.4794  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8700/8855]  eta: 0:08:54  lr: 0.000100  loss: 2.2468  time: 3.4500  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8750/8855]  eta: 0:06:01  lr: 0.000100  loss: 2.0775  time: 3.4399  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8800/8855]  eta: 0:03:09  lr: 0.000100  loss: 1.9781  time: 3.3986  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8850/8855]  eta: 0:00:17  lr: 0.000100  loss: 1.9174  time: 3.4309  data: 0.0000  max mem: 13061
Train: data epoch: [0]  [8854/8855]  eta: 0:00:03  lr: 0.000100  loss: 2.0702  time: 3.4605  data: 0.0000  max mem: 13061
Train: data epoch: [0] Total time: 8:28:42 (3.4470 s / it)
2023-08-15 20:09:49,220 [INFO] Averaged stats: lr: 0.0001  loss: 2.3202
2023-08-15 20:09:49,263 [INFO] No validation splits found.
2023-08-15 20:09:49,316 [INFO] Saving checkpoint at epoch 0 to /public/home/mswanghao/TorchProject/lavis/lavis/output/BLIP2/CEPretrain_stage2/20230815113/checkpoint_0.pth.
2023-08-15 20:09:53,413 [INFO] No validation splits found.
2023-08-15 20:09:53,437 [INFO] Training time 8:29:04
