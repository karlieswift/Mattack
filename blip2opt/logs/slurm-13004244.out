WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
loss DRSL3 b=1e-05 start=0 end=20loss DRSL3 b=1e-05 start=0 end=20

loss DRSL3 b=1e-05 start=0 end=20
loss DRSL3 b=1e-05 start=0 end=20
| distributed init (rank 2, world 4): env://
| distributed init (rank 3, world 4): env://
| distributed init (rank 1, world 4): env://| distributed init (rank 0, world 4): env://

[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
[W Module.cpp:513] Warning: Disabling benchmark mode for MIOpen is NOT supported. Overriding value to True (function operator())
2023-08-18 23:53:40,100 [INFO] 
=====  Running Parameters    =====
2023-08-18 23:53:40,100 [INFO] {
    "amp": true,
    "batch_size_eval": 2,
    "batch_size_train": 16,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 0.0005,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 10,
    "min_lr": 1e-05,
    "num_workers": 4,
    "output_dir": "output2/BLIP2/DRSL3_0_20_Pretrain_stage2",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "image_text_pretrain",
    "train_splits": [
        "train"
    ],
    "warmup_lr": 1e-06,
    "warmup_steps": 2000,
    "weight_decay": 0.05,
    "world_size": 4
}
2023-08-18 23:53:40,101 [INFO] 
======  Dataset Attributes  ======
2023-08-18 23:53:40,101 [INFO] 
======== coco_caption =======
2023-08-18 23:53:40,102 [INFO] {
    "build_info": {
        "annotations": {
            "test": {
                "md5": "3ff34b0ef2db02d01c37399f6a2a6cd1",
                "storage": "coco/annotations/coco_karpathy_test.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json"
            },
            "train": {
                "md5": "aa31ac474cf6250ebb81d18348a07ed8",
                "storage": "coco/annotations/coco_karpathy_train.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json"
            },
            "val": {
                "md5": "b273847456ef5580e33713b1f7de52a0",
                "storage": "coco/annotations/coco_karpathy_val.json",
                "url": "https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json"
            }
        },
        "images": {
            "storage": "coco/images/"
        }
    },
    "data_type": "images",
    "dataset_card": "dataset_card/coco_caption.md",
    "text_processor": {
        "train": {
            "name": "blip_caption"
        }
    },
    "vis_processor": {
        "train": {
            "image_size": 224,
            "name": "blip2_image_train"
        }
    }
}
2023-08-18 23:53:40,102 [INFO] 
======  Model Attributes  ======
2023-08-18 23:53:40,103 [INFO] {
    "arch": "blip2_opt",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 224,
    "load_finetuned": false,
    "load_pretrained": true,
    "model_type": "pretrain_opt2.7b",
    "num_query_token": 32,
    "opt_model": "facebook/opt-2.7b",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained.pth",
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_precision": "fp16"
}
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/coco/annotations/coco_karpathy_train.json
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/coco/annotations/coco_karpathy_val.json
Using downloaded and verified file: /public/home/mswanghao/.cache/lavis/coco/annotations/coco_karpathy_test.json
2023-08-18 23:53:40,111 [INFO] Building datasets...
2023-08-18 23:54:20,524 [INFO] freeze vision encoder
2023-08-18 23:57:42,559 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained.pth
2023-08-18 23:57:42,602 [INFO] Start training
2023-08-18 23:57:58,891 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-08-18 23:57:58,892 [INFO] Loaded 566747 records for train split from the dataset.
2023-08-18 23:57:58,892 [INFO] Loaded 5000 records for val split from the dataset.
2023-08-18 23:57:58,895 [INFO] Loaded 5000 records for test split from the dataset.
2023-08-18 23:57:58,964 [INFO] number of trainable parameters: 107133696
2023-08-18 23:57:58,968 [INFO] Start training epoch 0, 8855 iters per inner epoch.
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
MIOpen(HIP): Warning [ForwardBackwardDataGetWorkSpaceSizeWinograd] /MIOpen/src/sqlite_db.cpp:108: open memvfs: unable to open database file
Train: data epoch: [0]  [   0/8855]  eta: 2 days, 12:03:47  lr: 0.000001  loss: 6.2862  time: 24.4187  data: 0.0000  max mem: 11497
2023-08-18 23:58:23,449 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  50/8855]  eta: 10:42:18  lr: 0.000013  loss: 4.3272  time: 3.9644  data: 0.0000  max mem: 13577
Train: data epoch: [0]  [ 100/8855]  eta: 10:10:49  lr: 0.000026  loss: 3.8058  time: 4.0145  data: 0.0000  max mem: 13577
Train: data epoch: [0]  [ 150/8855]  eta: 9:57:11  lr: 0.000038  loss: 3.4293  time: 3.9390  data: 0.0000  max mem: 13577
Train: data epoch: [0]  [ 200/8855]  eta: 9:48:25  lr: 0.000051  loss: 2.6853  time: 3.9657  data: 0.0000  max mem: 13577
Train: data epoch: [0]  [ 250/8855]  eta: 9:42:16  lr: 0.000063  loss: 3.3490  time: 4.0149  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 300/8855]  eta: 9:36:53  lr: 0.000076  loss: 2.5438  time: 3.9844  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 350/8855]  eta: 9:32:15  lr: 0.000088  loss: 2.6766  time: 3.9798  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 400/8855]  eta: 9:27:40  lr: 0.000101  loss: 2.4018  time: 4.0164  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 450/8855]  eta: 9:23:56  lr: 0.000113  loss: 2.2949  time: 4.0350  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 500/8855]  eta: 9:19:48  lr: 0.000126  loss: 2.4620  time: 3.9915  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 550/8855]  eta: 9:15:35  lr: 0.000138  loss: 2.3922  time: 3.9632  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 600/8855]  eta: 9:12:00  lr: 0.000151  loss: 2.4756  time: 3.9758  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 650/8855]  eta: 9:08:22  lr: 0.000163  loss: 2.2258  time: 3.9596  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 700/8855]  eta: 9:04:43  lr: 0.000176  loss: 2.5801  time: 3.9696  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 750/8855]  eta: 9:01:09  lr: 0.000188  loss: 2.2943  time: 3.9779  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 800/8855]  eta: 8:57:36  lr: 0.000201  loss: 2.0906  time: 3.9600  data: 0.0000  max mem: 13579
Train: data epoch: [0]  [ 850/8855]  eta: 8:54:10  lr: 0.000213  loss: 2.7426  time: 3.9884  data: 0.0000  max mem: 13601
Train: data epoch: [0]  [ 900/8855]  eta: 8:50:58  lr: 0.000226  loss: 2.4503  time: 4.0271  data: 0.0000  max mem: 13601
Train: data epoch: [0]  [ 950/8855]  eta: 8:47:25  lr: 0.000238  loss: 2.2167  time: 3.9877  data: 0.0000  max mem: 13601
Train: data epoch: [0]  [1000/8855]  eta: 8:44:02  lr: 0.000251  loss: 2.2075  time: 3.9970  data: 0.0000  max mem: 13601
Train: data epoch: [0]  [1050/8855]  eta: 8:40:24  lr: 0.000263  loss: 2.3573  time: 3.9689  data: 0.0000  max mem: 13601
Train: data epoch: [0]  [1100/8855]  eta: 8:37:07  lr: 0.000275  loss: 2.6966  time: 4.0215  data: 0.0000  max mem: 13601
Train: data epoch: [0]  [1150/8855]  eta: 8:33:36  lr: 0.000288  loss: 2.1365  time: 3.9912  data: 0.0000  max mem: 13601
Train: data epoch: [0]  [1200/8855]  eta: 8:30:19  lr: 0.000300  loss: 2.8024  time: 4.0222  data: 0.0000  max mem: 13601
Train: data epoch: [0]  [1250/8855]  eta: 8:26:57  lr: 0.000313  loss: 2.2138  time: 3.9985  data: 0.0000  max mem: 13601
Train: data epoch: [0]  [1300/8855]  eta: 8:23:39  lr: 0.000325  loss: 2.8348  time: 3.9975  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1350/8855]  eta: 8:20:14  lr: 0.000338  loss: 2.6142  time: 3.9913  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1400/8855]  eta: 8:16:52  lr: 0.000350  loss: 2.2999  time: 4.0405  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1450/8855]  eta: 8:13:23  lr: 0.000363  loss: 2.2681  time: 3.9230  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1500/8855]  eta: 8:10:00  lr: 0.000375  loss: 3.0142  time: 3.9757  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1550/8855]  eta: 8:06:33  lr: 0.000388  loss: 2.7250  time: 3.9825  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1600/8855]  eta: 8:03:00  lr: 0.000400  loss: 3.2800  time: 3.8979  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1650/8855]  eta: 7:59:44  lr: 0.000413  loss: 2.6369  time: 4.0440  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1700/8855]  eta: 7:56:26  lr: 0.000425  loss: 2.3961  time: 4.0069  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1750/8855]  eta: 7:53:02  lr: 0.000438  loss: 2.5077  time: 3.9216  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1800/8855]  eta: 7:49:40  lr: 0.000450  loss: 2.4817  time: 3.9786  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1850/8855]  eta: 7:46:16  lr: 0.000463  loss: 2.5545  time: 3.9627  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1900/8855]  eta: 7:42:51  lr: 0.000475  loss: 2.5918  time: 3.9560  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [1950/8855]  eta: 7:39:37  lr: 0.000488  loss: 2.4812  time: 4.0378  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2000/8855]  eta: 7:36:14  lr: 0.000500  loss: 2.4906  time: 4.0181  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2050/8855]  eta: 7:32:53  lr: 0.000500  loss: 2.6492  time: 3.9663  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2100/8855]  eta: 7:29:35  lr: 0.000500  loss: 2.5018  time: 3.9857  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2150/8855]  eta: 7:26:10  lr: 0.000500  loss: 2.3458  time: 3.9553  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2200/8855]  eta: 7:22:45  lr: 0.000500  loss: 2.2005  time: 3.9427  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2250/8855]  eta: 7:19:22  lr: 0.000500  loss: 2.1171  time: 3.9730  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2300/8855]  eta: 7:16:05  lr: 0.000500  loss: 2.5622  time: 3.9965  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2350/8855]  eta: 7:12:45  lr: 0.000500  loss: 2.2331  time: 4.0176  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2400/8855]  eta: 7:09:32  lr: 0.000500  loss: 2.5532  time: 4.1003  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2450/8855]  eta: 7:06:18  lr: 0.000500  loss: 2.3335  time: 4.0016  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2500/8855]  eta: 7:02:58  lr: 0.000500  loss: 2.2437  time: 3.9872  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2550/8855]  eta: 6:59:38  lr: 0.000500  loss: 2.5496  time: 4.0235  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2600/8855]  eta: 6:56:16  lr: 0.000500  loss: 1.9803  time: 3.9477  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2650/8855]  eta: 6:52:58  lr: 0.000500  loss: 2.5125  time: 3.9666  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2700/8855]  eta: 6:49:42  lr: 0.000500  loss: 2.4317  time: 4.0358  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2750/8855]  eta: 6:46:23  lr: 0.000500  loss: 2.1897  time: 3.9512  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2800/8855]  eta: 6:43:03  lr: 0.000500  loss: 2.4121  time: 3.9503  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2850/8855]  eta: 6:39:44  lr: 0.000500  loss: 2.1087  time: 4.0212  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2900/8855]  eta: 6:36:21  lr: 0.000500  loss: 2.3909  time: 3.9593  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [2950/8855]  eta: 6:33:00  lr: 0.000500  loss: 2.0544  time: 4.0047  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3000/8855]  eta: 6:29:40  lr: 0.000500  loss: 2.5373  time: 3.9551  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3050/8855]  eta: 6:26:20  lr: 0.000500  loss: 2.6504  time: 3.9598  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3100/8855]  eta: 6:22:58  lr: 0.000500  loss: 2.4381  time: 3.9990  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3150/8855]  eta: 6:19:39  lr: 0.000500  loss: 2.1882  time: 3.9966  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3200/8855]  eta: 6:16:15  lr: 0.000500  loss: 2.2279  time: 3.9442  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3250/8855]  eta: 6:12:53  lr: 0.000500  loss: 2.0767  time: 3.9786  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3300/8855]  eta: 6:09:33  lr: 0.000500  loss: 2.4983  time: 3.9820  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3350/8855]  eta: 6:06:13  lr: 0.000500  loss: 2.1527  time: 3.9463  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3400/8855]  eta: 6:02:53  lr: 0.000500  loss: 2.0778  time: 3.9729  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3450/8855]  eta: 5:59:37  lr: 0.000500  loss: 2.3384  time: 4.0287  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3500/8855]  eta: 5:56:16  lr: 0.000500  loss: 2.2652  time: 3.9440  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3550/8855]  eta: 5:52:58  lr: 0.000500  loss: 2.2979  time: 4.0003  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3600/8855]  eta: 5:49:36  lr: 0.000500  loss: 2.4026  time: 3.9423  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3650/8855]  eta: 5:46:15  lr: 0.000500  loss: 2.4562  time: 3.9605  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3700/8855]  eta: 5:42:55  lr: 0.000500  loss: 2.4428  time: 3.9732  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3750/8855]  eta: 5:39:35  lr: 0.000500  loss: 2.4352  time: 3.9579  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3800/8855]  eta: 5:36:17  lr: 0.000500  loss: 2.5581  time: 4.0645  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3850/8855]  eta: 5:33:07  lr: 0.000500  loss: 2.4202  time: 4.1436  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3900/8855]  eta: 5:29:57  lr: 0.000500  loss: 2.0480  time: 4.1591  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [3950/8855]  eta: 5:26:47  lr: 0.000500  loss: 2.1051  time: 4.1723  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4000/8855]  eta: 5:23:39  lr: 0.000500  loss: 2.3953  time: 4.2107  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4050/8855]  eta: 5:20:27  lr: 0.000500  loss: 2.5932  time: 4.1443  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4100/8855]  eta: 5:17:19  lr: 0.000500  loss: 2.5430  time: 4.1966  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4150/8855]  eta: 5:14:08  lr: 0.000500  loss: 2.2413  time: 4.1848  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4200/8855]  eta: 5:10:56  lr: 0.000500  loss: 2.1113  time: 4.1212  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4250/8855]  eta: 5:07:45  lr: 0.000500  loss: 2.3347  time: 4.2125  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4300/8855]  eta: 5:04:32  lr: 0.000500  loss: 2.1543  time: 4.1787  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4350/8855]  eta: 5:01:20  lr: 0.000500  loss: 2.2651  time: 4.1725  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4400/8855]  eta: 4:58:06  lr: 0.000500  loss: 2.3936  time: 4.1401  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4450/8855]  eta: 4:54:55  lr: 0.000500  loss: 2.1270  time: 4.1555  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4500/8855]  eta: 4:51:41  lr: 0.000500  loss: 2.7511  time: 4.1435  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4550/8855]  eta: 4:48:27  lr: 0.000500  loss: 2.3632  time: 4.1505  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4600/8855]  eta: 4:45:13  lr: 0.000500  loss: 2.4409  time: 4.2027  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4650/8855]  eta: 4:41:58  lr: 0.000500  loss: 2.2354  time: 4.1514  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4700/8855]  eta: 4:38:44  lr: 0.000500  loss: 2.1176  time: 4.1791  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4750/8855]  eta: 4:35:32  lr: 0.000500  loss: 2.6384  time: 4.2041  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4800/8855]  eta: 4:32:16  lr: 0.000500  loss: 2.3063  time: 4.1111  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4850/8855]  eta: 4:28:59  lr: 0.000500  loss: 2.0351  time: 4.1884  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4900/8855]  eta: 4:25:43  lr: 0.000500  loss: 2.4794  time: 4.1638  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [4950/8855]  eta: 4:22:26  lr: 0.000500  loss: 2.1501  time: 4.1116  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5000/8855]  eta: 4:19:10  lr: 0.000500  loss: 2.0290  time: 4.1662  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5050/8855]  eta: 4:15:51  lr: 0.000500  loss: 2.4499  time: 4.1209  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5100/8855]  eta: 4:12:35  lr: 0.000500  loss: 2.1333  time: 4.1379  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5150/8855]  eta: 4:09:19  lr: 0.000500  loss: 2.4689  time: 4.1760  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5200/8855]  eta: 4:06:01  lr: 0.000500  loss: 2.3092  time: 4.1541  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5250/8855]  eta: 4:02:44  lr: 0.000500  loss: 2.1672  time: 4.0917  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5300/8855]  eta: 3:59:27  lr: 0.000500  loss: 2.2430  time: 4.2232  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5350/8855]  eta: 3:56:10  lr: 0.000500  loss: 2.1266  time: 4.1655  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5400/8855]  eta: 3:52:50  lr: 0.000500  loss: 2.0687  time: 4.1208  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5450/8855]  eta: 3:49:32  lr: 0.000500  loss: 2.1771  time: 4.1015  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5500/8855]  eta: 3:46:13  lr: 0.000500  loss: 2.2363  time: 4.1243  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5550/8855]  eta: 3:42:55  lr: 0.000500  loss: 2.0315  time: 4.1977  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5600/8855]  eta: 3:39:36  lr: 0.000500  loss: 2.1032  time: 4.1735  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5650/8855]  eta: 3:36:16  lr: 0.000500  loss: 2.1699  time: 4.1541  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5700/8855]  eta: 3:32:57  lr: 0.000500  loss: 2.3363  time: 4.1047  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5750/8855]  eta: 3:29:37  lr: 0.000500  loss: 2.3556  time: 4.1854  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5800/8855]  eta: 3:26:13  lr: 0.000500  loss: 1.9470  time: 4.0227  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5850/8855]  eta: 3:22:49  lr: 0.000500  loss: 2.3349  time: 3.9814  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5900/8855]  eta: 3:19:26  lr: 0.000500  loss: 1.8576  time: 4.0096  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [5950/8855]  eta: 3:16:02  lr: 0.000500  loss: 2.6605  time: 4.0399  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6000/8855]  eta: 3:12:39  lr: 0.000500  loss: 2.1391  time: 4.0365  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6050/8855]  eta: 3:09:17  lr: 0.000500  loss: 2.4840  time: 4.0775  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6100/8855]  eta: 3:05:53  lr: 0.000500  loss: 2.2369  time: 4.0246  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6150/8855]  eta: 3:02:30  lr: 0.000500  loss: 2.4941  time: 4.0422  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6200/8855]  eta: 2:59:07  lr: 0.000500  loss: 2.4320  time: 3.9580  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6250/8855]  eta: 2:55:44  lr: 0.000500  loss: 2.2875  time: 3.9938  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6300/8855]  eta: 2:52:21  lr: 0.000500  loss: 2.0893  time: 4.0626  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6350/8855]  eta: 2:48:57  lr: 0.000500  loss: 2.2709  time: 3.9538  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6400/8855]  eta: 2:45:35  lr: 0.000500  loss: 2.3690  time: 4.0110  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6450/8855]  eta: 2:42:13  lr: 0.000500  loss: 2.4298  time: 4.0108  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6500/8855]  eta: 2:38:50  lr: 0.000500  loss: 2.3110  time: 4.0461  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6550/8855]  eta: 2:35:27  lr: 0.000500  loss: 2.7007  time: 4.0099  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6600/8855]  eta: 2:32:04  lr: 0.000500  loss: 2.1852  time: 4.0659  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6650/8855]  eta: 2:28:41  lr: 0.000500  loss: 2.2958  time: 3.9879  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6700/8855]  eta: 2:25:18  lr: 0.000500  loss: 2.1677  time: 4.0099  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6750/8855]  eta: 2:21:56  lr: 0.000500  loss: 2.5610  time: 4.1033  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6800/8855]  eta: 2:18:34  lr: 0.000500  loss: 2.1225  time: 4.0426  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6850/8855]  eta: 2:15:12  lr: 0.000500  loss: 2.0046  time: 4.0173  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6900/8855]  eta: 2:11:49  lr: 0.000500  loss: 1.9822  time: 3.9824  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [6950/8855]  eta: 2:08:26  lr: 0.000500  loss: 2.3540  time: 4.0638  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7000/8855]  eta: 2:05:04  lr: 0.000500  loss: 2.1095  time: 3.9838  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7050/8855]  eta: 2:01:41  lr: 0.000500  loss: 2.2429  time: 4.0368  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7100/8855]  eta: 1:58:19  lr: 0.000500  loss: 2.1452  time: 4.0576  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7150/8855]  eta: 1:54:55  lr: 0.000500  loss: 2.5351  time: 3.9650  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7200/8855]  eta: 1:51:33  lr: 0.000500  loss: 2.0504  time: 3.9915  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7250/8855]  eta: 1:48:10  lr: 0.000500  loss: 2.1268  time: 4.0256  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7300/8855]  eta: 1:44:48  lr: 0.000500  loss: 2.3470  time: 4.0127  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7350/8855]  eta: 1:41:25  lr: 0.000500  loss: 2.5251  time: 4.0240  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7400/8855]  eta: 1:38:03  lr: 0.000500  loss: 2.4016  time: 4.0011  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7450/8855]  eta: 1:34:40  lr: 0.000500  loss: 2.4201  time: 3.9668  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7500/8855]  eta: 1:31:18  lr: 0.000500  loss: 2.5078  time: 4.0099  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7550/8855]  eta: 1:27:55  lr: 0.000500  loss: 2.4518  time: 3.9926  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7600/8855]  eta: 1:24:33  lr: 0.000500  loss: 2.4949  time: 4.0576  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7650/8855]  eta: 1:21:10  lr: 0.000500  loss: 2.2757  time: 3.9576  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7700/8855]  eta: 1:17:48  lr: 0.000500  loss: 2.4042  time: 3.9685  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7750/8855]  eta: 1:14:25  lr: 0.000500  loss: 1.9968  time: 4.0248  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7800/8855]  eta: 1:11:03  lr: 0.000500  loss: 2.2554  time: 3.9934  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7850/8855]  eta: 1:07:41  lr: 0.000500  loss: 2.1779  time: 4.0231  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7900/8855]  eta: 1:04:19  lr: 0.000500  loss: 2.0320  time: 4.0026  data: 0.0000  max mem: 13608
Train: data epoch: [0]  [7950/8855]  eta: 1:00:56  lr: 0.000500  loss: 2.4267  time: 4.0196  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8000/8855]  eta: 0:57:34  lr: 0.000500  loss: 2.2171  time: 4.0266  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8050/8855]  eta: 0:54:12  lr: 0.000500  loss: 2.4999  time: 4.0070  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8100/8855]  eta: 0:50:50  lr: 0.000500  loss: 2.3077  time: 3.9923  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8150/8855]  eta: 0:47:28  lr: 0.000500  loss: 1.9909  time: 4.0217  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8200/8855]  eta: 0:44:06  lr: 0.000500  loss: 2.2337  time: 4.0598  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8250/8855]  eta: 0:40:44  lr: 0.000500  loss: 2.1193  time: 3.9782  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8300/8855]  eta: 0:37:22  lr: 0.000500  loss: 2.1773  time: 3.9958  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8350/8855]  eta: 0:34:00  lr: 0.000500  loss: 2.5031  time: 4.0006  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8400/8855]  eta: 0:30:38  lr: 0.000500  loss: 2.0895  time: 4.0829  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8450/8855]  eta: 0:27:16  lr: 0.000500  loss: 2.0330  time: 3.9981  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8500/8855]  eta: 0:23:54  lr: 0.000500  loss: 2.3632  time: 4.0575  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8550/8855]  eta: 0:20:31  lr: 0.000500  loss: 2.3750  time: 4.0015  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8600/8855]  eta: 0:17:09  lr: 0.000500  loss: 2.0077  time: 4.0251  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8650/8855]  eta: 0:13:48  lr: 0.000500  loss: 2.3312  time: 4.0795  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8700/8855]  eta: 0:10:26  lr: 0.000500  loss: 2.3868  time: 3.9958  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8750/8855]  eta: 0:07:04  lr: 0.000500  loss: 2.2999  time: 4.0334  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8800/8855]  eta: 0:03:42  lr: 0.000500  loss: 2.0945  time: 4.0037  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8850/8855]  eta: 0:00:20  lr: 0.000500  loss: 2.0064  time: 4.0065  data: 0.0000  max mem: 13623
Train: data epoch: [0]  [8854/8855]  eta: 0:00:04  lr: 0.000500  loss: 2.1453  time: 4.0170  data: 0.0000  max mem: 13623
Train: data epoch: [0] Total time: 9:56:03 (4.0387 s / it)
2023-08-19 09:54:02,051 [INFO] Averaged stats: lr: 0.0004  loss: 2.3716
2023-08-19 09:54:02,118 [INFO] No validation splits found.
2023-08-19 09:54:02,175 [INFO] Saving checkpoint at epoch 0 to /public/home/mswanghao/TorchProject/lavis/lavis/output2/BLIP2/DRSL3_0_20_Pretrain_stage2/20230818235/checkpoint_0.pth.
2023-08-19 09:54:05,833 [INFO] Start training
2023-08-19 09:54:05,879 [INFO] Start training epoch 1, 8855 iters per inner epoch.
Train: data epoch: [1]  [   0/8855]  eta: 20:22:25  lr: 0.000488  loss: 2.0848  time: 8.2829  data: 0.0000  max mem: 13623
slurmstepd: error: *** JOB 13004244 ON b06r1n11 CANCELLED AT 2023-08-19T09:56:38 ***
