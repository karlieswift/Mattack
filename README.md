# Distribution-restrained Softmax Loss for the Safety of Large Language Models
[Author]()* ,   

 
This repository contains the code and data for the following paper:  
<a href='https://github.com/karlieswift/Mattack'><img src='https://img.shields.io/badge/Project-Page-Green'></a>  <a href='https://arxiv.org'><img src='https://img.shields.io/badge/Paper-PDF-red'></a>  
## Introduction

The rapid development of multimodal models has greatly improved the development of artificial intelligence, especially the rapid development of large models, which has improved the rapid development of various fields. However, the robustness of multimodal models is indeed weak. This article proposes a relatively robust loss function by attacking multimodal models([vitgpt2](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning) [blip2opt](https://github.com/salesforce/LAVIS/tree/main/lavis/models/blip2_models)).

**The project was evaluated using two multimodal models**

For the imagecaption model, we used [vitgpt2](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)  
For the VQA model, we used [blip2opt](https://github.com/salesforce/LAVIS/tree/main/lavis/models/blip2_models)

**The detailed description of adversarial attacks in VQA [blip2opt/README.md](blip2opt/README.md).**  
**The detailed description of adversarial attacks in image captions  [vitgpt2/README.md](vitgpt2/README.md)**  
 



